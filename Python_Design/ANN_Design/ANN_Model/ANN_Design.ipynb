{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "\n",
    "This file serves to host an ANN model created for use in multi-robot task allocation (MRTA). This model performs regression based on the load history of a robot, its distance to the current task, as well as the total distance that it has travelled thus far. The goal for designing this ANN is to comapre its performance against an ANFIS to determine which is better at approximating an FIS. This will be achieved through comparing the coefficient of determination ($R^{2}$), root mean squared error (RMSE), and mean absolute error (MAE).\n",
    "\n",
    "**Date Created:** 18/12/2024\n",
    "\n",
    "**Date Modified:** 6/1/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages:**\n",
    "\n",
    "This section imports all necessary pacakges for the ANN implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Loading:**\n",
    "\n",
    "This section loads the data that was generated from the FIS. Minimal discovery is performed here, as the bulk of the data discovery was performed within the first grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# get the path to the data CSV:\n",
    "files_in_dir = os.listdir(os.getcwd())\n",
    "data_path = os.path.join(os.getcwd(), files_in_dir[files_in_dir.index('V3_Data.csv')])\n",
    "\n",
    "# load the CSV as a pandas dataframe:\n",
    "df = pd.read_csv(data_path)\n",
    "print(f'data successfully loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Pre-Processing:**\n",
    "\n",
    "This section will split the data into training, validation, and testing, alongside performing some pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature and label dataframes:\n",
    "x_data = df.drop(['Suitability'], axis = 1)\n",
    "y_data = df['Suitability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to first standardize the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a scaler:\n",
    "scaler = StandardScaler()\n",
    "x_data = scaler.fit_transform(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation, and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 12000 training examples\n",
      "there are 1500 validation examples\n",
      "there are 1500 testing examples\n"
     ]
    }
   ],
   "source": [
    "# split dataset:\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size = 0.5)\n",
    "\n",
    "# get split results:\n",
    "print(f\"there are {x_train.shape[0]} training examples\")\n",
    "print(f\"there are {x_val.shape[0]} validation examples\")\n",
    "print(f\"there are {x_test.shape[0]} testing examples\")\n",
    "\n",
    "# get input shape:\n",
    "INPUT_SHAPE = x_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create Model**\n",
    "\n",
    "Need to define the relevant metrics and loss functions, and then define a function for creating models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FUNCTION = 'mse'\n",
    "METRICS = ['mae', keras.metrics.RootMeanSquaredError(), keras.metrics.R2Score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model generation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(layers, neurons, rate, norm, drop):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (INPUT_SHAPE, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(1, activation = 'linear', name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = LOSS_FUNCTION,\n",
    "                  metrics = METRICS)\n",
    "    \n",
    "    return model \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
