{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "\n",
    "This file serves to compare the inference systems that had been developed.\n",
    "\n",
    "**Date Created: 16/2/2025**\n",
    "\n",
    "**Date Modified: 18/2/2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages:**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from pickle import load\n",
    "from ANFIS_Custom_Layers import *\n",
    "from PythonFISFunctionV3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-amble:**\n",
    "\n",
    "This section defines paths and variables to be used in the model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ANN directory path:\n",
    "ann_path = os.path.join(os.getcwd(), 'ANN_Model')\n",
    "\n",
    "# define the ANFIS directory path:\n",
    "anfis_path = os.path.join(os.getcwd(), 'ANFIS_Model')\n",
    "\n",
    "# define the dictionary of custom objects for the ANFIS:\n",
    "custom_objects = {\n",
    "    # # layers:\n",
    "    'MF_Layer'          : MF_Layer,\n",
    "    'FS_Layer'          : FS_Layer,\n",
    "    'NM_Layer'          : NM_Layer,\n",
    "    'CN_Layer'          : CN_Layer,\n",
    "    'O_Layer'           : O_Layer,\n",
    "\n",
    "    # other:\n",
    "    'OrderedConstraint' : OrderedConstraint(),\n",
    "    'mse'               : MeanSquaredError()\n",
    "}\n",
    "\n",
    "# define the batch size for model warming:\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create FIS:**\n",
    "\n",
    "This section creates the rule-base for the FIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulebase = fis_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load the ANN Model:**\n",
    "\n",
    "This section loads the ANN model and the scaler that is used for the ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "c:\\Users\\mtidd2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.33442]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the ann model:\n",
    "ann_model = load_model(ann_path + '/ann_model.h5', custom_objects = {'mse' : MeanSquaredError()})\n",
    "\n",
    "# load the ann scaler:\n",
    "ann_scaler = load(open(ann_path + '/ann_scaler.pkl', 'rb'))\n",
    "\n",
    "# need to warm the model to prevent retracing of computational graph on first inference:\n",
    "dummy_input = np.zeros((batch_size, 3), dtype = np.float32)\n",
    "dummy_input = ann_scaler.transform(dummy_input)\n",
    "ann_model.predict(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load the ANFIS Model:**\n",
    "\n",
    "This section loads the ANFIS model and the scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mtidd2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mtidd2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.028122]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model:\n",
    "anfis_model = load_model(anfis_path + '/anfis_model.h5', custom_objects = custom_objects)\n",
    "anfis_model.compile(optimizer=\"adam\", loss=\"mse\") \n",
    "\n",
    "# load the scaler:\n",
    "anfis_scaler = load(open(anfis_path + '/anfis_scaler.pkl', 'rb'))\n",
    "\n",
    "# warmup the model:\n",
    "dummy_input = np.zeros((batch_size, 3), dtype=np.float32)  \n",
    "dummy_input = anfis_scaler.transform(dummy_input)\n",
    "anfis_model.predict(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Testing Data:**\n",
    "\n",
    "This section generates the data that the models are tested on. This is done by sampling the universe of discourse of each variable and predicting the suitability using the FIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize testing data:\n",
    "test_data = []\n",
    "\n",
    "# generate data:\n",
    "for i in range(100):\n",
    "    # randomly sample each universe of discourse:\n",
    "    lh = float(np.random.randint(0, 10 + 1))\n",
    "    dtt = np.random.uniform(0, 25)\n",
    "    dh = np.random.uniform(0, 50)\n",
    "\n",
    "    # infer suitability, get time:\n",
    "    fis_start_time = time.time()\n",
    "    suit = fis_solve(rulebase, lh, dtt, dh)\n",
    "    fis_time = time.time() - fis_start_time\n",
    "\n",
    "    # append to test data:\n",
    "    test_data.append([lh, dtt, dh, suit, fis_time])\n",
    "\n",
    "# convert to numpy array:\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total inference time for the FIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total inference time was: 8.333 seconds\n",
      "average inference time was: 0.083 seconds\n"
     ]
    }
   ],
   "source": [
    "# get total inference time:\n",
    "total_fis_time = np.sum(test_data[:, -1])\n",
    "\n",
    "# get the average inference time:\n",
    "avg_fis_time = np.average(test_data[:, -1])\n",
    "\n",
    "# print to user:\n",
    "print(f'total inference time was: {round(total_fis_time, 3)} seconds')\n",
    "print(f'average inference time was: {round(avg_fis_time, 3)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test the Models:**\n",
    "\n",
    "This section tests the models against one another on the same input dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtidd2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# scale the inputs:\n",
    "scaled_inputs = ann_scaler.transform(test_data[:, :3])\n",
    "\n",
    "# results:\n",
    "ann_results = []\n",
    "\n",
    "# do inference:\n",
    "for a, b, c in scaled_inputs:\n",
    "    # formulate input:\n",
    "    input = np.array([[a, b, c]])\n",
    "\n",
    "    # get start time:\n",
    "    ann_start_time = time.time()\n",
    "    ann_prediction = ann_model.predict(input, verbose = 0)\n",
    "    ann_time = time.time() - ann_start_time\n",
    "\n",
    "    # append to results:\n",
    "    ann_results.append([*ann_prediction.flatten(), ann_time])\n",
    "\n",
    "# convert to numpy array:\n",
    "ann_results = np.array(ann_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total inference time for the ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total inference time was: 2.839 seconds\n",
      "average inference time was: 0.028 seconds\n"
     ]
    }
   ],
   "source": [
    "# get total inference time:\n",
    "total_ann_time = np.sum(ann_results[:, -1])\n",
    "\n",
    "# get the average inference time:\n",
    "avg_ann_time = np.average(ann_results[:, -1])\n",
    "\n",
    "# print to user:\n",
    "print(f'total inference time was: {round(total_ann_time, 3)} seconds')\n",
    "print(f'average inference time was: {round(avg_ann_time, 3)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the ANFIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the inputs:\n",
    "scaled_inputs = anfis_scaler.transform(test_data[:, :3])\n",
    "\n",
    "# results:\n",
    "anfis_results = []\n",
    "\n",
    "# do inference:\n",
    "for a, b, c in scaled_inputs:\n",
    "    # formulate input:\n",
    "    input = np.array([[a, b, c]])\n",
    "\n",
    "    # get start time:\n",
    "    anfis_start_time = time.time()\n",
    "    anfis_prediction = anfis_model.predict(input, verbose = 0)\n",
    "    anfis_time = time.time() - anfis_start_time\n",
    "\n",
    "    # append to results:\n",
    "    anfis_results.append([*anfis_prediction.flatten(), anfis_time])\n",
    "\n",
    "# convert to numpy array:\n",
    "anfis_results = np.array(anfis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total inference time for the ANFIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total inference time was: 2.911 seconds\n",
      "average inference time was: 0.029 seconds\n"
     ]
    }
   ],
   "source": [
    "# get total inference time:\n",
    "total_anfis_time = np.sum(anfis_results[:, -1])\n",
    "\n",
    "# get the average inference time:\n",
    "avg_anfis_time = np.average(anfis_results[:, -1])\n",
    "\n",
    "# print to user:\n",
    "print(f'total inference time was: {round(total_anfis_time, 3)} seconds')\n",
    "print(f'average inference time was: {round(avg_anfis_time, 3)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analyze the Results:**\n",
    "\n",
    "This section analyzes the results from this analysis. It compares the total inference time, the average inference time, the MAE, MSE, RMSE, and the $R^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the suitability of each model:\n",
    "fis_suits = test_data[:, -2]\n",
    "ann_suits = ann_results[:, 0]\n",
    "anfis_suits = anfis_results[:, 0]\n",
    "\n",
    "# calculate the difference between the values:\n",
    "percentage_diff_ann = (ann_suits - fis_suits) / fis_suits * 100\n",
    "percentage_diff_anfis = (anfis_suits - fis_suits) / fis_suits * 100\n",
    "\n",
    "# mean of the absolute percentage differences:\n",
    "avg_percent_diff_ann = np.mean(np.abs((ann_suits - fis_suits) / fis_suits * 100))\n",
    "avg_percent_diff_anfis = np.mean(np.abs((anfis_suits - fis_suits) / fis_suits * 100))\n",
    "\n",
    "# metrics for ann:\n",
    "ann_mae = mean_absolute_error(fis_suits, ann_suits)\n",
    "ann_mse = mean_squared_error(fis_suits, ann_suits)\n",
    "ann_rmse = root_mean_squared_error(fis_suits, ann_suits)\n",
    "ann_r2 = r2_score(fis_suits, ann_suits)\n",
    "\n",
    "# amalgamate:\n",
    "ann_metrics = [ann_mae, ann_mse, ann_rmse, ann_r2, total_ann_time, avg_ann_time]\n",
    "\n",
    "# metrics for anfis:\n",
    "anfis_mae = mean_absolute_error(fis_suits, anfis_suits)\n",
    "anfis_mse = mean_squared_error(fis_suits, anfis_suits)\n",
    "anfis_rmse = root_mean_squared_error(fis_suits, anfis_suits)\n",
    "anfis_r2 = r2_score(fis_suits, anfis_suits)\n",
    "\n",
    "# amalgamate:\n",
    "anfis_metrics = [anfis_mae, anfis_mse, anfis_rmse, anfis_r2, total_anfis_time, avg_anfis_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data for comparing these suitabilities:\n",
    "data1 = {'FIS Suitability' : fis_suits, \n",
    "        'ANN Suitability' : ann_suits, \n",
    "        'ANN-FIS % Difference' : percentage_diff_ann, \n",
    "        'ANFIS Suitability' : anfis_suits, \n",
    "        'ANFIS-FIS % Difference' : percentage_diff_anfis}\n",
    "\n",
    "# define the dataframe for suitability differences:\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# define the data for comparing the metrics:\n",
    "data2 = {'ANN Metrics' : ann_metrics, 'ANFIS Metrics' : anfis_metrics}\n",
    "\n",
    "# define the dataframe for metrics:\n",
    "df2 = pd.DataFrame(data2)\n",
    "df2.index = ['MAE', 'MSE', 'RMSE', 'R2', 'Total Time (s)', 'Avg. Time (s)']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results to user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics are: \n",
      "                 ANN Metrics  ANFIS Metrics\n",
      "MAE                0.063542       0.262236\n",
      "MSE                0.006630       0.095840\n",
      "RMSE               0.081426       0.309581\n",
      "R2                 0.996683       0.952046\n",
      "Total Time (s)     2.838912       2.911282\n",
      "Avg. Time (s)      0.028389       0.029113\n",
      "\n",
      "mean of the absolute percentage difference for ANN: 1.999 %\n",
      "mean of the absolute percentage difference for ANFIS: 8.221 %\n",
      "\n",
      "the total time taken by the FIS for inference was: 8.333 seconds\n",
      "the average time taken by the FIS for inference was: 0.083 seconds\n",
      "\n",
      "this results in a 65.931 % reduction for the ANN, and a 65.062 % reduction for the ANFIS\n"
     ]
    }
   ],
   "source": [
    "# print the metrics:\n",
    "print(f'metrics are: \\n {df2}')\n",
    "\n",
    "# average percent differences:\n",
    "print(f'\\nmean of the absolute percentage difference for ANN: {round(avg_percent_diff_ann, 3)} %')\n",
    "print(f'mean of the absolute percentage difference for ANFIS: {round(avg_percent_diff_anfis, 3)} %')\n",
    "\n",
    "# print out the FIS results:\n",
    "print(f'\\nthe total time taken by the FIS for inference was: {round(total_fis_time, 3)} seconds')\n",
    "print(f'the average time taken by the FIS for inference was: {round(avg_fis_time, 3)} seconds\\n')\n",
    "print(f'this results in a {round((1 - avg_ann_time/avg_fis_time) * 100, 3)} % reduction for the ANN, and a {round((1 - avg_anfis_time/avg_fis_time) * 100, 3)} % reduction for the ANFIS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
