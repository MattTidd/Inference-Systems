{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "\n",
    "This file serves to compare the inference systems that had been developed.\n",
    "\n",
    "**Date Created: 16/2/2025**\n",
    "\n",
    "**Date Modified: 18/2/2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages:**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from pickle import load\n",
    "from ANFIS_Custom_Layers import *\n",
    "from PythonFISFunctionV3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-amble:**\n",
    "\n",
    "This section defines paths and variables to be used in the model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ANN directory path:\n",
    "ann_path = os.path.join(os.getcwd(), 'ANN_Model')\n",
    "\n",
    "# define the ANFIS directory path:\n",
    "anfis_path = os.path.join(os.getcwd(), 'ANFIS_Model')\n",
    "\n",
    "# define the dictionary of custom objects for the ANFIS:\n",
    "custom_objects = {\n",
    "    # # layers:\n",
    "    'MF_Layer'          : MF_Layer,\n",
    "    'FS_Layer'          : FS_Layer,\n",
    "    'NM_Layer'          : NM_Layer,\n",
    "    'CN_Layer'          : CN_Layer,\n",
    "    'O_Layer'           : O_Layer,\n",
    "\n",
    "    # other:\n",
    "    'OrderedConstraint' : OrderedConstraint(),\n",
    "    'mse'               : MeanSquaredError()\n",
    "}\n",
    "\n",
    "# define the batch size for model warming:\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create FIS:**\n",
    "\n",
    "This section creates the rule-base for the FIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulebase = fis_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load the ANN Model:**\n",
    "\n",
    "This section loads the ANN model and the scaler that is used for the ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtidd2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.33442]], dtype=float32)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the ann model:\n",
    "ann_model = load_model(ann_path + '/ann_model.h5', custom_objects = {'mse' : MeanSquaredError()})\n",
    "\n",
    "# load the ann scaler:\n",
    "ann_scaler = load(open(ann_path + '/ann_scaler.pkl', 'rb'))\n",
    "\n",
    "# need to warm the model to prevent retracing of computational graph on first inference:\n",
    "dummy_input = np.zeros((batch_size, 3), dtype = np.float32)\n",
    "dummy_input = ann_scaler.transform(dummy_input)\n",
    "ann_model.predict(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load the ANFIS Model:**\n",
    "\n",
    "This section loads the ANFIS model and the scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.028122]], dtype=float32)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model:\n",
    "anfis_model = load_model(anfis_path + '/anfis_model.h5', custom_objects = custom_objects)\n",
    "anfis_model.compile(optimizer=\"adam\", loss=\"mse\") \n",
    "\n",
    "# load the scaler:\n",
    "anfis_scaler = load(open(anfis_path + '/anfis_scaler.pkl', 'rb'))\n",
    "\n",
    "# warmup the model:\n",
    "dummy_input = np.zeros((batch_size, 3), dtype=np.float32)  \n",
    "dummy_input = anfis_scaler.transform(dummy_input)\n",
    "anfis_model.predict(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Testing Data:**\n",
    "\n",
    "This section generates the data that the models are tested on. This is done by sampling the universe of discourse of each variable and predicting the suitability using the FIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize testing data:\n",
    "test_data = []\n",
    "\n",
    "# generate data:\n",
    "for i in range(100):\n",
    "    # randomly sample each universe of discourse:\n",
    "    lh = float(np.random.randint(0, 10 + 1))\n",
    "    dtt = np.random.uniform(0, 25)\n",
    "    dh = np.random.uniform(0, 50)\n",
    "\n",
    "    # infer suitability, get time:\n",
    "    fis_start_time = time.time()\n",
    "    suit = fis_solve(rulebase, lh, dtt, dh)\n",
    "    fis_time = time.time() - fis_start_time\n",
    "\n",
    "    # append to test data:\n",
    "    test_data.append([lh, dtt, dh, suit, fis_time])\n",
    "\n",
    "# convert to numpy array:\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total inference time for the FIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total inference time was: 9.067 seconds\n",
      "average inference time was: 0.091 seconds\n"
     ]
    }
   ],
   "source": [
    "# get total inference time:\n",
    "total_fis_time = np.sum(test_data[:, -1])\n",
    "\n",
    "# get the average inference time:\n",
    "avg_fis_time = np.average(test_data[:, -1])\n",
    "\n",
    "# print to user:\n",
    "print(f'total inference time was: {round(total_fis_time, 3)} seconds')\n",
    "print(f'average inference time was: {round(avg_fis_time, 3)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test the Models:**\n",
    "\n",
    "This section tests the models against one another on the same input dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtidd2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# scale the inputs:\n",
    "scaled_inputs = ann_scaler.transform(test_data[:, :3])\n",
    "\n",
    "# results:\n",
    "ann_results = []\n",
    "\n",
    "# do inference:\n",
    "for a, b, c in scaled_inputs:\n",
    "    # formulate input:\n",
    "    input = np.array([[a, b, c]])\n",
    "\n",
    "    # get start time:\n",
    "    ann_start_time = time.time()\n",
    "    ann_prediction = ann_model.predict(input, verbose = 0)\n",
    "    ann_time = time.time() - ann_start_time\n",
    "\n",
    "    # append to results:\n",
    "    ann_results.append([*ann_prediction.flatten(), ann_time])\n",
    "\n",
    "# convert to numpy array:\n",
    "ann_results = np.array(ann_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total inference time for the ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total inference time was: 3.996 seconds\n",
      "average inference time was: 0.04 seconds\n"
     ]
    }
   ],
   "source": [
    "# get total inference time:\n",
    "total_ann_time = np.sum(ann_results[:, -1])\n",
    "\n",
    "# get the average inference time:\n",
    "avg_ann_time = np.average(ann_results[:, -1])\n",
    "\n",
    "# print to user:\n",
    "print(f'total inference time was: {round(total_ann_time, 3)} seconds')\n",
    "print(f'average inference time was: {round(avg_ann_time, 3)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the ANFIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the inputs:\n",
    "scaled_inputs = anfis_scaler.transform(test_data[:, :3])\n",
    "\n",
    "# results:\n",
    "anfis_results = []\n",
    "\n",
    "# do inference:\n",
    "for a, b, c in scaled_inputs:\n",
    "    # formulate input:\n",
    "    input = np.array([[a, b, c]])\n",
    "\n",
    "    # get start time:\n",
    "    anfis_start_time = time.time()\n",
    "    anfis_prediction = anfis_model.predict(input, verbose = 0)\n",
    "    anfis_time = time.time() - anfis_start_time\n",
    "\n",
    "    # append to results:\n",
    "    anfis_results.append([*anfis_prediction.flatten(), anfis_time])\n",
    "\n",
    "# convert to numpy array:\n",
    "anfis_results = np.array(anfis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total inference time for the ANFIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total inference time was: 4.041 seconds\n",
      "average inference time was: 0.04 seconds\n"
     ]
    }
   ],
   "source": [
    "# get total inference time:\n",
    "total_anfis_time = np.sum(anfis_results[:, -1])\n",
    "\n",
    "# get the average inference time:\n",
    "avg_anfis_time = np.average(anfis_results[:, -1])\n",
    "\n",
    "# print to user:\n",
    "print(f'total inference time was: {round(total_anfis_time, 3)} seconds')\n",
    "print(f'average inference time was: {round(avg_anfis_time, 3)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analyze the Results:**\n",
    "\n",
    "This section analyzes the results from this analysis. It compares the total inference time, the average inference time, the MAE, MSE, RMSE, and the $R^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the suitability of each model:\n",
    "fis_suits = test_data[:, -2]\n",
    "ann_suits = ann_results[:, 0]\n",
    "anfis_suits = anfis_results[:, 0]\n",
    "\n",
    "# calculate the difference between the values:\n",
    "percentage_diff_ann = (ann_suits - fis_suits) / fis_suits * 100\n",
    "percentage_diff_anfis = (anfis_suits - fis_suits) / fis_suits * 100\n",
    "\n",
    "# mean of the absolute percentage differences:\n",
    "avg_percent_diff_ann = np.mean(np.abs((ann_suits - fis_suits) / fis_suits * 100))\n",
    "avg_percent_diff_anfis = np.mean(np.abs((anfis_suits - fis_suits) / fis_suits * 100))\n",
    "\n",
    "# metrics for ann:\n",
    "ann_mae = mean_absolute_error(fis_suits, ann_suits)\n",
    "ann_mse = mean_squared_error(fis_suits, ann_suits)\n",
    "ann_rmse = root_mean_squared_error(fis_suits, ann_suits)\n",
    "ann_r2 = r2_score(fis_suits, ann_suits)\n",
    "\n",
    "# amalgamate:\n",
    "ann_metrics = [ann_mae, ann_mse, ann_rmse, ann_r2, total_ann_time, avg_ann_time]\n",
    "\n",
    "# metrics for anfis:\n",
    "anfis_mae = mean_absolute_error(fis_suits, anfis_suits)\n",
    "anfis_mse = mean_squared_error(fis_suits, anfis_suits)\n",
    "anfis_rmse = root_mean_squared_error(fis_suits, anfis_suits)\n",
    "anfis_r2 = r2_score(fis_suits, anfis_suits)\n",
    "\n",
    "# amalgamate:\n",
    "anfis_metrics = [anfis_mae, anfis_mse, anfis_rmse, anfis_r2, total_anfis_time, avg_anfis_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data for comparing these suitabilities:\n",
    "data1 = {'FIS Suitability' : fis_suits, \n",
    "        'ANN Suitability' : ann_suits, \n",
    "        'ANN-FIS % Difference' : percentage_diff_ann, \n",
    "        'ANFIS Suitability' : anfis_suits, \n",
    "        'ANFIS-FIS % Difference' : percentage_diff_anfis}\n",
    "\n",
    "# define the dataframe for suitability differences:\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# define the data for comparing the metrics:\n",
    "data2 = {'ANN Metrics' : ann_metrics, 'ANFIS Metrics' : anfis_metrics}\n",
    "\n",
    "# define the dataframe for metrics:\n",
    "df2 = pd.DataFrame(data2)\n",
    "df2.index = ['MAE', 'MSE', 'RMSE', 'R2', 'Total Time (s)', 'Avg. Time (s)']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results to user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics are: \n",
      "                 ANN Metrics  ANFIS Metrics\n",
      "MAE                0.076185       0.239555\n",
      "MSE                0.011472       0.086502\n",
      "RMSE               0.107108       0.294113\n",
      "R2                 0.995246       0.964156\n",
      "Total Time (s)     3.996010       4.040599\n",
      "Avg. Time (s)      0.039960       0.040406\n",
      "\n",
      " mean of the absolute percentage difference for ANN: 2.288 %\n",
      "\n",
      " mean of the absolute percentage difference for ANFIS: 7.29 %\n"
     ]
    }
   ],
   "source": [
    "# print the metrics:\n",
    "print(f'metrics are: \\n {df2}')\n",
    "\n",
    "# average percent differences:\n",
    "print(f'\\n mean of the absolute percentage difference for ANN: {round(avg_percent_diff_ann, 3)} %')\n",
    "print(f'\\n mean of the absolute percentage difference for ANFIS: {round(avg_percent_diff_anfis, 3)} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
