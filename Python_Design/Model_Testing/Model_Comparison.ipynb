{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "\n",
    "This file serves to compare the inference systems that had been developed.\n",
    "\n",
    "**Date Created: 16/2/2025**\n",
    "\n",
    "**Date Modified: 18/2/2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages:**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from pickle import load\n",
    "from ANFIS_Custom_Layers import *\n",
    "from PythonFISFunctionV3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-amble:**\n",
    "\n",
    "This section defines paths and variables to be used in the model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ANN directory path:\n",
    "ann_path = os.path.join(os.getcwd(), 'ANN_Model')\n",
    "\n",
    "# define the ANFIS directory path:\n",
    "anfis_path = os.path.join(os.getcwd(), 'ANFIS_Model')\n",
    "\n",
    "# define the dictionary of custom objects for the ANFIS:\n",
    "custom_objects = {\n",
    "    # # layers:\n",
    "    'MF_Layer'          : MF_Layer,\n",
    "    'FS_Layer'          : FS_Layer,\n",
    "    'NM_Layer'          : NM_Layer,\n",
    "    'CN_Layer'          : CN_Layer,\n",
    "    'O_Layer'           : O_Layer,\n",
    "\n",
    "    # other:\n",
    "    'OrderedConstraint' : OrderedConstraint(),\n",
    "    'mse'               : MeanSquaredError()\n",
    "}\n",
    "\n",
    "# define the batch size for model warming:\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create FIS:**\n",
    "\n",
    "This section creates the rule-base for the FIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulebase = fis_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load the ANN Model:**\n",
    "\n",
    "This section loads the ANN model and the scaler that is used for the ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtidd2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.33442]], dtype=float32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the ann model:\n",
    "ann_model = load_model(ann_path + '/ann_model.h5', custom_objects = {'mse' : MeanSquaredError()})\n",
    "\n",
    "# load the ann scaler:\n",
    "ann_scaler = load(open(ann_path + '/ann_scaler.pkl', 'rb'))\n",
    "\n",
    "# need to warm the model to prevent retracing of computational graph on first inference:\n",
    "dummy_input = np.zeros((batch_size, 3), dtype = np.float32)\n",
    "dummy_input = ann_scaler.transform(dummy_input)\n",
    "ann_model.predict(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load the ANFIS Model:**\n",
    "\n",
    "This section loads the ANFIS model and the scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.028122]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model:\n",
    "anfis_model = load_model(anfis_path + '/anfis_model.h5', custom_objects = custom_objects)\n",
    "anfis_model.compile(optimizer=\"adam\", loss=\"mse\") \n",
    "\n",
    "# load the scaler:\n",
    "anfis_scaler = load(open(anfis_path + '/anfis_scaler.pkl', 'rb'))\n",
    "\n",
    "# warmup the model:\n",
    "dummy_input = np.zeros((batch_size, 3), dtype=np.float32)  \n",
    "dummy_input = anfis_scaler.transform(dummy_input)\n",
    "anfis_model.predict(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Testing Data:**\n",
    "\n",
    "This section generates the data that the models are tested on. This is done by sampling the universe of discourse of each variable and predicting the suitability using the FIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize testing data:\n",
    "test_data = []\n",
    "\n",
    "# generate data:\n",
    "for i in range(100):\n",
    "    # randomly sample each universe of discourse:\n",
    "    lh = float(np.random.randint(0, 10 + 1))\n",
    "    dtt = np.random.uniform(0, 25)\n",
    "    dh = np.random.uniform(0, 50)\n",
    "\n",
    "    # infer suitability, get time:\n",
    "    fis_start_time = time.time()\n",
    "    suit = fis_solve(rulebase, lh, dtt, dh)\n",
    "    fis_time = time.time() - fis_start_time\n",
    "\n",
    "    # append to test data:\n",
    "    test_data.append([lh, dtt, dh, suit, fis_time])\n",
    "\n",
    "# convert to numpy array:\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total inference time for the FIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total inference time was: 9.313 seconds\n",
      "average inference time was: 0.093 seconds\n"
     ]
    }
   ],
   "source": [
    "# get total inference time:\n",
    "total_fis_time = np.sum(test_data[:, -1])\n",
    "\n",
    "# get the average inference time:\n",
    "avg_fis_time = np.average(test_data[:, -1])\n",
    "\n",
    "# print to user:\n",
    "print(f'total inference time was: {round(total_fis_time, 3)} seconds')\n",
    "print(f'average inference time was: {round(avg_fis_time, 3)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test the Models:**\n",
    "\n",
    "This section tests the models against one another on the same input dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtidd2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# scale the inputs:\n",
    "scaled_inputs = ann_scaler.transform(test_data[:, :3])\n",
    "\n",
    "# results:\n",
    "ann_results = []\n",
    "\n",
    "# do inference:\n",
    "for a, b, c in scaled_inputs:\n",
    "    # formulate input:\n",
    "    input = np.array([[a, b, c]])\n",
    "\n",
    "    # get start time:\n",
    "    ann_start_time = time.time()\n",
    "    ann_prediction = ann_model.predict(input, verbose = 0)\n",
    "    ann_time = time.time() - ann_start_time\n",
    "\n",
    "    # append to results:\n",
    "    ann_results.append([*ann_prediction.flatten(), ann_time])\n",
    "\n",
    "# convert to numpy array:\n",
    "ann_results = np.array(ann_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total inference time for the ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total inference time was: 4.122 seconds\n",
      "average inference time was: 0.041 seconds\n"
     ]
    }
   ],
   "source": [
    "# get total inference time:\n",
    "total_ann_time = np.sum(ann_results[:, -1])\n",
    "\n",
    "# get the average inference time:\n",
    "avg_ann_time = np.average(ann_results[:, -1])\n",
    "\n",
    "# print to user:\n",
    "print(f'total inference time was: {round(total_ann_time, 3)} seconds')\n",
    "print(f'average inference time was: {round(avg_ann_time, 3)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the ANFIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the inputs:\n",
    "scaled_inputs = anfis_scaler.transform(test_data[:, :3])\n",
    "\n",
    "# results:\n",
    "anfis_results = []\n",
    "\n",
    "# do inference:\n",
    "for a, b, c in scaled_inputs:\n",
    "    # formulate input:\n",
    "    input = np.array([[a, b, c]])\n",
    "\n",
    "    # get start time:\n",
    "    anfis_start_time = time.time()\n",
    "    anfis_prediction = anfis_model.predict(input, verbose = 0)\n",
    "    anfis_time = time.time() - anfis_start_time\n",
    "\n",
    "    # append to results:\n",
    "    anfis_results.append([*anfis_prediction.flatten(), anfis_time])\n",
    "\n",
    "# convert to numpy array:\n",
    "anfis_results = np.array(anfis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total inference time for the ANFIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total inference time was: 4.155 seconds\n",
      "average inference time was: 0.042 seconds\n"
     ]
    }
   ],
   "source": [
    "# get total inference time:\n",
    "total_anfis_time = np.sum(anfis_results[:, -1])\n",
    "\n",
    "# get the average inference time:\n",
    "avg_anfis_time = np.average(anfis_results[:, -1])\n",
    "\n",
    "# print to user:\n",
    "print(f'total inference time was: {round(total_anfis_time, 3)} seconds')\n",
    "print(f'average inference time was: {round(avg_anfis_time, 3)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analyze the Results:**\n",
    "\n",
    "This section analyzes the results from this analysis. It compares the total inference time, the average inference time, the MAE, MSE, RMSE, and the $R^{2}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
