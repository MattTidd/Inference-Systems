{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "\n",
    "This file serves to compare the inference systems that had been developed.\n",
    "\n",
    "**Date Created: 16/2/2025**\n",
    "\n",
    "**Date Modified: 18/2/2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages:**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pickle import load\n",
    "from ANFIS_Custom_Layers import *\n",
    "from PythonFISFunctionV3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-amble:**\n",
    "\n",
    "This section defines paths and variables to be used in the model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ANN directory path:\n",
    "ann_path = os.path.join(os.getcwd(), 'ANN_Model')\n",
    "\n",
    "# define the ANFIS directory path:\n",
    "anfis_path = os.path.join(os.getcwd(), 'ANFIS_Model')\n",
    "\n",
    "# define the dictionary of custom objects for the ANFIS:\n",
    "custom_objects = {\n",
    "    # # layers:\n",
    "    'MF_Layer'          : MF_Layer,\n",
    "    'FS_Layer'          : FS_Layer,\n",
    "    'NM_Layer'          : NM_Layer,\n",
    "    'CN_Layer'          : CN_Layer,\n",
    "    'O_Layer'           : O_Layer,\n",
    "\n",
    "    # other:\n",
    "    'OrderedConstraint' : OrderedConstraint(),\n",
    "    'mse'               : MeanSquaredError()\n",
    "}\n",
    "\n",
    "# define the batch size for model warming:\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create FIS:**\n",
    "\n",
    "This section creates the rule-base for the FIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulebase = fis_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load the ANN Model:**\n",
    "\n",
    "This section loads the ANN model and the scaler that is used for the ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000275F0FB6AC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtidd2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000275F0FB6AC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.33442]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the ann model:\n",
    "ann_model = load_model(ann_path + '/ann_model.h5', custom_objects = {'mse' : MeanSquaredError()})\n",
    "\n",
    "# load the ann scaler:\n",
    "ann_scaler = load(open(ann_path + '/ann_scaler.pkl', 'rb'))\n",
    "\n",
    "# need to warm the model to prevent retracing of computational graph on first inference:\n",
    "dummy_input = np.zeros((batch_size, 3), dtype = np.float32)\n",
    "dummy_input = ann_scaler.transform(dummy_input)\n",
    "ann_model.predict(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load the ANFIS Model:**\n",
    "\n",
    "This section loads the ANFIS model and the scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.028122]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model:\n",
    "anfis_model = load_model(anfis_path + '/anfis_model.h5', custom_objects = custom_objects)\n",
    "anfis_model.compile(optimizer=\"adam\", loss=\"mse\") \n",
    "\n",
    "# load the scaler:\n",
    "anfis_scaler = load(open(anfis_path + '/anfis_scaler.pkl', 'rb'))\n",
    "\n",
    "# warmup the model:\n",
    "dummy_input = np.zeros((batch_size, 3), dtype=np.float32)  \n",
    "dummy_input = anfis_scaler.transform(dummy_input)\n",
    "anfis_model.predict(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Testing Data:**\n",
    "\n",
    "This section generates the data that the models are tested on. This is done by sampling the universe of discourse of each variable and predicting the suitability using the FIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize testing data:\n",
    "test_data = []\n",
    "\n",
    "# generate data:\n",
    "for i in range(100):\n",
    "    lh = float(np.random.randint(0, 10 + 1))\n",
    "    dtt = np.random.uniform(0, 25)\n",
    "    dh = np.random.uniform(0, 50)\n",
    "\n",
    "    suit = fis_solve(rulebase, lh, dtt, dh)\n",
    "    test_data.append([lh, dtt, dh, suit])\n",
    "\n",
    "# convert to numpy array:\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test the Models:**\n",
    "\n",
    "This section tests the models against one another on the same input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the ANN:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
