{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "\n",
    "This file utilizes FISV3 to generate data that will be used to train an \n",
    "adaptive neuro-fuzzy inference system (ANFIS).\n",
    "\n",
    "This process involves generating data randomly within the universe of \n",
    "discourse of each variable, and using this generated data to infer the \n",
    "suitability of a given robot. This is appended to a dataframe and exported\n",
    "as a CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages:**\n",
    "\n",
    "Need to import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PythonFISFunctionV3 import *\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Generation Settings:**\n",
    "\n",
    "Define the following parameters for data generation, such as the number of iterations required, as well as the max value for each input variables' universe of discourse. \n",
    "\n",
    "The Pandas DataFrame and FIS rulebase are also instantiated here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 15000  # max iterations\n",
    "max_ud_load = 10    # max value for the load history universe of discourse\n",
    "max_ud_dtt = 25     # max value for the distance to task universe of discourse\n",
    "max_ud_tdt = 50     # max value for the total distance travelled universe of discourse\n",
    "\n",
    "# instantiate rulebase:\n",
    "rulebase = fis_create()\n",
    "\n",
    "# instantiate dataframe:\n",
    "columns = ['Load History', 'Distance to Task', 'Total Distance Travelled', 'Suitability']\n",
    "df = pd.DataFrame(np.zeros((iterations, len(columns))), columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Generation Loop:**\n",
    "\n",
    "This is the main data generation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15000/15000\r"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    # randomly generate three robot parameters:\n",
    "    load = np.random.randint(0, max_ud_load + 1)\n",
    "    distance = np.random.randint(0, max_ud_dtt + 1)\n",
    "    travelled = np.random.randint(0, max_ud_tdt + 1)\n",
    "\n",
    "    # calculate suitability:\n",
    "    suit = fis_solve(rulebase, load, distance, travelled)\n",
    "\n",
    "    # create next row, append to df:\n",
    "    next_row = [load, distance, travelled, suit]\n",
    "    df.iloc[i] = next_row\n",
    "    print(f\"iteration {i+1}/{iterations}\", end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Manipulation and Export:**\n",
    "\n",
    "In this section, the data is pre-split into seperate columns, and exported. It is split here into training and testing values that can be used within MATLAB, but the entire DataFrame is also exported such that it can be split into the desired train/test splits for later use in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training x values: (12000, 3), training y values: (12000,)\n",
      "validation x values: (1500, 3), validation y values: (1500,)\n",
      "testing x values: (1500, 3), training y values: (1500,)\n"
     ]
    }
   ],
   "source": [
    "# export entire dataframe as a CSV:\n",
    "cwd = os.getcwd()\n",
    "df.to_csv(cwd + '/Data/V3_Data.csv', index = False)\n",
    "\n",
    "# split into train and test dataframes:\n",
    "x_data = df.drop(columns = 'Suitability')\n",
    "y_data = df['Suitability']\n",
    "input_train, input_val_test, output_train, output_val_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "input_val, input_test, output_val, output_test = train_test_split(input_val_test, output_val_test, test_size = 0.5)\n",
    "\n",
    "\n",
    "print(f\"training x values: {input_train.shape}, training y values: {output_train.shape}\")\n",
    "print(f\"validation x values: {input_val.shape}, validation y values: {output_val.shape}\")\n",
    "print(f\"testing x values: {input_test.shape}, training y values: {output_test.shape}\")\n",
    "\n",
    "# export these as CSVs:\n",
    "input_train.to_csv(cwd + '/Data/train_input.csv', index = False)\n",
    "output_train.to_csv(cwd + '/Data/train_output.csv', index = False)\n",
    "input_val.to_csv(cwd + '/Data/val_input.csv', index = False)\n",
    "output_val.to_csv(cwd + '/Data/val_output.csv', index = False)\n",
    "input_test.to_csv(cwd + '/Data/test_input.csv', index = False)\n",
    "output_test.to_csv(cwd + '/Data/test_output.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
