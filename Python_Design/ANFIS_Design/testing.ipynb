{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing keras custom layer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, constraints\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomizer seed:\n",
    "np.random.seed(0)\n",
    "\n",
    "# first layer -> membership layer:\n",
    "class MF_Layer(Layer): \n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(MF_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "\n",
    "        # need to initialize antecedent parameters\n",
    "        self.mf_params = self.add_weight(\n",
    "            shape = (self.num_inputs, self.num_mfs, 2),             \n",
    "            initializer= tf.keras.initializers.RandomUniform(0.0, 50.0),\n",
    "            trainable = True,\n",
    "            name = 'Antecedent_Params'\n",
    "        )\n",
    "\n",
    "    # custom setting of weights:\n",
    "    def set_weights(self, params):\n",
    "        # this function is used to set weights based on what a user provides\n",
    "        # user must provide weights in the form of a np.array of shape (num_mfs, num_params)\n",
    "\n",
    "        if params.shape != (self.num_inputs, self.num_mfs, 3):\n",
    "            raise ValueError(f'Parameters provided are not of correct shape, expected ({self.num_inputs}, {self.num_mfs}, 3)')\n",
    "\n",
    "        self.mf_params = params\n",
    "        \n",
    "    # function call:\n",
    "    def call(self, inputs):\n",
    "        # need to initialize the membership values:\n",
    "        membership_values = []\n",
    "\n",
    "        # for every input:\n",
    "        for i in range(self.num_inputs):\n",
    "            # get the memberships for that input:\n",
    "            input_mf_params = self.mf_params[i]\n",
    "\n",
    "            # need to now compute the fuzzified value for each membership function:\n",
    "            fuzzified_values = []\n",
    "\n",
    "            # for every membership function:\n",
    "            for j in range(self.num_mfs):\n",
    "                mean = input_mf_params[j, 0]  # Mean of the Gaussian\n",
    "                std = input_mf_params[j, 1]   # Standard deviation of the Gaussian\n",
    "                output = tf.exp(-0.5 * tf.square((inputs[:, i] - mean) / (std + 1e-6)))\n",
    "                fuzzified_values.append(output)\n",
    "            \n",
    "            # need to now stack the mf values for that given input:\n",
    "            membership_values.append(tf.stack(fuzzified_values, axis = -1))\n",
    "\n",
    "        # stack everything and return:\n",
    "        return tf.stack(membership_values, axis = 1)\n",
    "\n",
    "# second layer -> firing strength layer:\n",
    "class FS_Layer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(FS_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, membership_values):\n",
    "        # this layer accepts the membership values, which have shape (batch_size, num_inputs, num_mfs):\n",
    "        batch_size = tf.shape(membership_values)[0]\n",
    "\n",
    "        # initialize the firing strengths:\n",
    "        firing_strengths = tf.ones((batch_size, self.num_rules), dtype = tf.float32)\n",
    "\n",
    "        # generate all the rule combinations:\n",
    "        rules = list(product(range(self.num_mfs), repeat = self.num_inputs))    # example [(0, 0, 0), (0, 0, 1), ...]\n",
    "\n",
    "        # need to check each input, each mf combination, and multiply their values together:\n",
    "        for rule_index, combination in enumerate(rules):\n",
    "            # print(f'combination: {combination}')\n",
    "            rule_strength = tf.ones((batch_size, ), dtype = tf.float32)\n",
    "\n",
    "            # for every input and membership function:\n",
    "            for input_index, mf_index in enumerate(combination):\n",
    "                # print(f'input: {input_index + 1} | mf: {mf_index + 1}')\n",
    "\n",
    "                # correctly extract the fuzzified values based on the combination index:\n",
    "                rule_strength *= membership_values[:, input_index, mf_index] + 1e-6\n",
    "            \n",
    "            # update the firing strengths:\n",
    "            rule_strength = tf.expand_dims(rule_strength, axis = -1)  # shape: (batch_size, 1)\n",
    "            firing_strengths = tf.concat(\n",
    "                [firing_strengths[:, :rule_index], rule_strength, firing_strengths[:, rule_index + 1:]],\n",
    "                axis = 1,\n",
    "            )\n",
    "            # print(f'firing strength: {firing_strengths}')\n",
    "\n",
    "        return firing_strengths\n",
    "    \n",
    "# third layer -> normalization layer:\n",
    "class NM_Layer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(NM_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, firing_strengths):\n",
    "        # this function accepts inputs of size (batch_size, num_rules).\n",
    "        # need to first get the total firing strength:\n",
    "        total_firing_strength = tf.reduce_sum(firing_strengths, axis = 1, keepdims = True)\n",
    "        \n",
    "        # can now normalize the firing strengths:\n",
    "        normalized_strengths = firing_strengths / (total_firing_strength + 1e-10)   # add a buffer in case the total firing strength is zero\n",
    "\n",
    "        return normalized_strengths\n",
    "    \n",
    "# fourth layer -> consequent layer:\n",
    "class CN_Layer(Layer):\n",
    "    # constructor: \n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(CN_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "        # need to initialize the consequent parameters:\n",
    "        self.consequent_params = self.add_weight(\n",
    "            shape = (self.num_rules, self.num_inputs + 1),\n",
    "            initializer = tf.keras.initializers.RandomUniform(-1.0, 1.0, seed = 1234),\n",
    "            trainable = True,\n",
    "            name = 'Consequent_Params'\n",
    "        )\n",
    "\n",
    "    # this function is used for manually setting the consequent parameters:\n",
    "    def set_cons(self, params):\n",
    "        # this function accepts parameters as an array of size (num_rules, num_inputs + 1):\n",
    "        if params.shape != (self.num_rules, self.num_inputs + 1):\n",
    "            raise ValueError(f'Parameters provided are not of correct shape, expected ({self.num_rules}, {self.num_inputs + 1})')\n",
    "        \n",
    "        # assign parameters:\n",
    "        self.consequent_params = params\n",
    "\n",
    "    # call function:\n",
    "    def call(self, input_list):\n",
    "        # unpack inputs from list:\n",
    "        normalized_strengths, inputs = input_list\n",
    "\n",
    "        # get the batch size:\n",
    "        batch_size = tf.shape(normalized_strengths)[0]\n",
    "\n",
    "        # the output is given by the multiplication of the inputs with the consequent weights,\n",
    "        # such as: o_k = w_bar_k * (x_1 * p_k + x_2 * q_k + x_3 * r_k + ... + s_k)\n",
    "        # can therefore extend the inputs to be (batch_size, num_inputs + bias) for ease of multiplication:\n",
    "        inputs_with_bias = tf.concat([inputs, tf.ones((batch_size, 1), dtype = tf.float32)], axis = -1)\n",
    "\n",
    "        # need to now reshape the normalized strengths to be of size (batch_size, num_rules, 1)\n",
    "        # this effectively flips it into a 'column vector' of sorts, where each individual value is now vertically aligned\n",
    "        normalized_strengths = tf.reshape(normalized_strengths, (batch_size, self.num_rules, 1))\n",
    "\n",
    "        # get the consequent parameters, which have shape (num_rules, num_inputs + 1):\n",
    "        consequent_params = self.consequent_params\n",
    "\n",
    "        # expand inputs with bias to match the rule axis: (batch_size, num_rules, num_inputs + 1)\n",
    "        inputs_with_bias_expanded = tf.expand_dims(inputs_with_bias, axis = 1)\n",
    "\n",
    "        # calculate the consequent for each rule\n",
    "        consequents = tf.reduce_sum(normalized_strengths * inputs_with_bias_expanded * consequent_params, axis = 2)\n",
    "\n",
    "        return consequents\n",
    "\n",
    "# fifth layer -> output layer:\n",
    "class O_Layer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(O_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_output = num_mfs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, consequents):\n",
    "        output = tf.reduce_sum(consequents, axis = 1, keepdims = True)\n",
    "        return output\n",
    "    \n",
    "# define a custom function for building models:\n",
    "def BuildAnfis(input_shape, num_inputs, num_mfs, rate):\n",
    "    # define the inputs:\n",
    "    inputs = Input(shape = input_shape)\n",
    "\n",
    "    # add the custom layers:\n",
    "    membership_layer = MF_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(inputs)\n",
    "    firing_layer = FS_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(membership_layer)\n",
    "    normalization_layer = NM_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(firing_layer)\n",
    "    consequent_layer = CN_Layer(num_inputs = num_inputs, num_mfs = num_mfs)([normalization_layer, inputs])\n",
    "    output_layer = O_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(consequent_layer)\n",
    "\n",
    "    # create and compile the model:\n",
    "    model = Model(inputs = inputs, outputs = output_layer)\n",
    "    model.compile(optimizer = Adam(learning_rate = rate, clipvalue = 1.0), \n",
    "                  loss = 'mse', \n",
    "                  metrics = ['mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "values for debugging the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the testing parameters that I am using for the membership functions:\n",
    "params = tf.constant(np.array([\n",
    "    [  # Parameters for input 1\n",
    "        [0.0, 0.0, 6.0],\n",
    "        [5/6, 5.0, 55/6],\n",
    "        [4.0, 10.0, 10.0]\n",
    "    ],\n",
    "    [  # Parameters for input 2\n",
    "        [0.0, 0.0 , 15.0],\n",
    "        [25/12, 12.5, 275/12],\n",
    "        [10.0, 25.0, 25.0]\n",
    "    ],\n",
    "    [  # Parameters for input 3\n",
    "        [0.0, 0.0, 30.0],\n",
    "        [25/6, 25.0, 275/6],\n",
    "        [15.0, 50.0, 50.0]\n",
    "    ]\n",
    "]), dtype = tf.float32)\n",
    "\n",
    "# these are testing parameters used for debugging:\n",
    "cons_params = tf.ones(shape = (27, 4), dtype = tf.float32)\n",
    "\n",
    "# these are testing inputs used for debugging:\n",
    "input = tf.constant([[8, 5, 32]], dtype = tf.float32)\n",
    "# input = tf.constant([[8, 5, 32], [2, 17, 22]], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to now import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# import data from csv as pandas dataframe:\n",
    "data = pd.read_csv('V3_Data.csv')\n",
    "print('data loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into x and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform split:\n",
    "x_data = data.drop(columns = 'Suitability').astype('float32').values\n",
    "y_data = data['Suitability'].astype('float32').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to now split into training, validation, and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training examples have shape: (8000, 3)\n",
      "validation examples have shape: (1000, 3)\n",
      "testing examples have shape:(1000, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the data using train_test_split:\n",
    "x_train, x_filler, y_train, y_filler = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_filler, y_filler, test_size = 0.5)\n",
    "\n",
    "# get the split results:\n",
    "print(f'training examples have shape: {x_train.shape}')\n",
    "print(f'validation examples have shape: {x_val.shape}')\n",
    "print(f'testing examples have shape:{x_test.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scale data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a scaler:\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale each set:\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# save the scaler:\n",
    "# dump(scaler, open('scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[25.342762 , 44.296967 ],\n",
       "        [26.42932  , 24.463713 ],\n",
       "        [43.366306 , 38.32352  ]],\n",
       "\n",
       "       [[46.28632  , 27.829016 ],\n",
       "        [21.633678 , 10.039205 ],\n",
       "        [32.406746 , 20.076477 ]],\n",
       "\n",
       "       [[41.04966  ,  3.381163 ],\n",
       "        [ 1.548475 ,  4.3872538],\n",
       "        [49.17117  ,  9.121096 ]]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make model:\n",
    "tf.keras.backend.clear_session()\n",
    "model = BuildAnfis((3,), 3, 3, rate = 0.001)\n",
    "model.layers[1].mf_params.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 14.7516 - mae: 3.5473 - root_mean_squared_error: 3.8400 - val_loss: 12.4627 - val_mae: 3.3075 - val_root_mean_squared_error: 3.5303\n",
      "Epoch 2/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 12.0092 - mae: 3.2547 - root_mean_squared_error: 3.4651 - val_loss: 10.3036 - val_mae: 3.0584 - val_root_mean_squared_error: 3.2099\n",
      "Epoch 3/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 10.0545 - mae: 3.0303 - root_mean_squared_error: 3.1705 - val_loss: 8.4988 - val_mae: 2.8139 - val_root_mean_squared_error: 2.9153\n",
      "Epoch 4/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 8.3204 - mae: 2.7848 - root_mean_squared_error: 2.8840 - val_loss: 6.9916 - val_mae: 2.5756 - val_root_mean_squared_error: 2.6442\n",
      "Epoch 5/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 6.7986 - mae: 2.5405 - root_mean_squared_error: 2.6071 - val_loss: 5.7176 - val_mae: 2.3432 - val_root_mean_squared_error: 2.3911\n",
      "Epoch 6/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 5.5609 - mae: 2.3104 - root_mean_squared_error: 2.3578 - val_loss: 4.6341 - val_mae: 2.1159 - val_root_mean_squared_error: 2.1527\n",
      "Epoch 7/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 4.4338 - mae: 2.0679 - root_mean_squared_error: 2.1055 - val_loss: 3.7078 - val_mae: 1.8934 - val_root_mean_squared_error: 1.9256\n",
      "Epoch 8/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 3.5489 - mae: 1.8496 - root_mean_squared_error: 1.8836 - val_loss: 2.9165 - val_mae: 1.6758 - val_root_mean_squared_error: 1.7078\n",
      "Epoch 9/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 2.7776 - mae: 1.6323 - root_mean_squared_error: 1.6664 - val_loss: 2.2506 - val_mae: 1.4655 - val_root_mean_squared_error: 1.5002\n",
      "Epoch 10/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 2.1681 - mae: 1.4339 - root_mean_squared_error: 1.4721 - val_loss: 1.7046 - val_mae: 1.2661 - val_root_mean_squared_error: 1.3056\n",
      "Epoch 11/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 1.6169 - mae: 1.2284 - root_mean_squared_error: 1.2714 - val_loss: 1.2651 - val_mae: 1.0788 - val_root_mean_squared_error: 1.1248\n",
      "Epoch 12/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 1.1909 - mae: 1.0422 - root_mean_squared_error: 1.0911 - val_loss: 0.9187 - val_mae: 0.9047 - val_root_mean_squared_error: 0.9585\n",
      "Epoch 13/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.8731 - mae: 0.8754 - root_mean_squared_error: 0.9341 - val_loss: 0.6535 - val_mae: 0.7462 - val_root_mean_squared_error: 0.8084\n",
      "Epoch 14/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 0.6113 - mae: 0.7157 - root_mean_squared_error: 0.7817 - val_loss: 0.4572 - val_mae: 0.6070 - val_root_mean_squared_error: 0.6762\n",
      "Epoch 15/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 0.4302 - mae: 0.5833 - root_mean_squared_error: 0.6557 - val_loss: 0.3187 - val_mae: 0.4916 - val_root_mean_squared_error: 0.5646\n",
      "Epoch 16/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 0.3074 - mae: 0.4761 - root_mean_squared_error: 0.5543 - val_loss: 0.2257 - val_mae: 0.4044 - val_root_mean_squared_error: 0.4751\n",
      "Epoch 17/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 0.2253 - mae: 0.3970 - root_mean_squared_error: 0.4744 - val_loss: 0.1673 - val_mae: 0.3437 - val_root_mean_squared_error: 0.4090\n",
      "Epoch 18/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.1688 - mae: 0.3405 - root_mean_squared_error: 0.4107 - val_loss: 0.1331 - val_mae: 0.3031 - val_root_mean_squared_error: 0.3649\n",
      "Epoch 19/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 0.1356 - mae: 0.3025 - root_mean_squared_error: 0.3681 - val_loss: 0.1149 - val_mae: 0.2793 - val_root_mean_squared_error: 0.3389\n",
      "Epoch 20/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 0.1179 - mae: 0.2810 - root_mean_squared_error: 0.3433 - val_loss: 0.1063 - val_mae: 0.2660 - val_root_mean_squared_error: 0.3261\n",
      "Epoch 21/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.1113 - mae: 0.2721 - root_mean_squared_error: 0.3336 - val_loss: 0.1029 - val_mae: 0.2595 - val_root_mean_squared_error: 0.3208\n",
      "Epoch 22/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 0.1073 - mae: 0.2675 - root_mean_squared_error: 0.3275 - val_loss: 0.1017 - val_mae: 0.2564 - val_root_mean_squared_error: 0.3189\n",
      "Epoch 23/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - loss: 0.1079 - mae: 0.2664 - root_mean_squared_error: 0.3284 - val_loss: 0.1015 - val_mae: 0.2554 - val_root_mean_squared_error: 0.3187\n",
      "Epoch 24/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.1061 - mae: 0.2640 - root_mean_squared_error: 0.3256 - val_loss: 0.1015 - val_mae: 0.2547 - val_root_mean_squared_error: 0.3185\n",
      "Epoch 25/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.1052 - mae: 0.2620 - root_mean_squared_error: 0.3244 - val_loss: 0.1014 - val_mae: 0.2544 - val_root_mean_squared_error: 0.3185\n",
      "Epoch 26/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 0.1081 - mae: 0.2658 - root_mean_squared_error: 0.3287 - val_loss: 0.1016 - val_mae: 0.2547 - val_root_mean_squared_error: 0.3188\n",
      "Epoch 27/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 0.1050 - mae: 0.2616 - root_mean_squared_error: 0.3241 - val_loss: 0.1015 - val_mae: 0.2544 - val_root_mean_squared_error: 0.3186\n",
      "Epoch 28/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 0.1060 - mae: 0.2633 - root_mean_squared_error: 0.3255 - val_loss: 0.1015 - val_mae: 0.2545 - val_root_mean_squared_error: 0.3186\n",
      "Epoch 29/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 0.1051 - mae: 0.2637 - root_mean_squared_error: 0.3242 - val_loss: 0.1017 - val_mae: 0.2548 - val_root_mean_squared_error: 0.3189\n",
      "Epoch 30/500\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 0.1070 - mae: 0.2635 - root_mean_squared_error: 0.3271 - val_loss: 0.1015 - val_mae: 0.2543 - val_root_mean_squared_error: 0.3185\n"
     ]
    }
   ],
   "source": [
    "# train the damn model:\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "history = model.fit(x_train, y_train, validation_data = (x_val, y_val), batch_size = 32, epochs = 500, callbacks = early_stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot history:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
