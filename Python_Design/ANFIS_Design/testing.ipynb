{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing keras custom layer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import os \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, regularizers, constraints, layers, optimizers\n",
    "from keras.layers import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomizer seed:\n",
    "np.random.seed(0)\n",
    "\n",
    "# need to define a constraint for training the parameters:\n",
    "class OrderedConstraint(constraints.Constraint):\n",
    "    # constructor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # call function for constraint:\n",
    "    def __call__(self, W):\n",
    "        return tf.sort(W, axis = 2)\n",
    "\n",
    "# first layer -> membership layer:\n",
    "class MF_Layer(Layer): \n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(MF_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "\n",
    "        # need to initialize antecedent parameters, but a <= b <= c. therefore \n",
    "        # generate a set of \"raw parameters\" to be sorted. these are not trained\n",
    "\n",
    "        raw_params = np.random.uniform(low = 0.0, high = 50.0, size = (self.num_inputs, self.num_mfs, 3))\n",
    "        sorted_params = tf.sort(raw_params, axis = -1)\n",
    "\n",
    "        self.mf_params = self.add_weight(\n",
    "            shape = (self.num_inputs, self.num_mfs, 3),             # num_inputs, num_mfs per input, 3 params per mf (triangular)\n",
    "            initializer = tf.constant_initializer(sorted_params.numpy()),\n",
    "            trainable = True,\n",
    "            constraint = OrderedConstraint(),\n",
    "            name = 'Antecedent_Params'\n",
    "        )\n",
    "\n",
    "    # custom setting of weights:\n",
    "    def set_weights(self, params):\n",
    "        # this function is used to set weights based on what a user provides\n",
    "        # user must provide weights in the form of a np.array of shape (num_mfs, num_params)\n",
    "\n",
    "        if params.shape != (self.num_inputs, self.num_mfs, 3):\n",
    "            raise ValueError(f'Parameters provided are not of correct shape, expected ({self.num_inputs}, {self.num_mfs}, 3)')\n",
    "\n",
    "        self.mf_params = params\n",
    "        \n",
    "    # function call:\n",
    "    def call(self, inputs):\n",
    "        # need to initialize the membership values:\n",
    "        membership_values = []\n",
    "\n",
    "        # for every input:\n",
    "        for i in range(self.num_inputs):\n",
    "            # get the memberships for that input:\n",
    "            input_mf_params = self.mf_params[i]\n",
    "\n",
    "            # need to now compute the fuzzified value for each membership function:\n",
    "            fuzzified_values = []\n",
    "\n",
    "            # for every membership function:\n",
    "            for j in range(self.num_mfs):\n",
    "                a = input_mf_params[j, 0]\n",
    "                b = input_mf_params[j, 1]\n",
    "                c = input_mf_params[j, 2]\n",
    "\n",
    "                # check to see if we are on the edges:\n",
    "                is_left_edge = (j == 0) & (a == b)                  # this would be the left-most ramp\n",
    "                is_right_edge = (j == self.num_mfs - 1) & (b == c)  # this would be the right-most ramp\n",
    "        \n",
    "                # ramp calculations:\n",
    "                left = tf.where((inputs[:, i] == a) & is_left_edge, 1.0, (inputs[:, i] - a) / (b - a))\n",
    "                right = tf.where((inputs[:, i] == c) & is_right_edge, 1.0, (c - inputs[:, i]) / (c - b))\n",
    "\n",
    "                output = tf.maximum(0.0, tf.minimum(left, right))\n",
    "\n",
    "                fuzzified_values.append(output)\n",
    "        \n",
    "            # need to now stack the mf values for that given input:\n",
    "            membership_values.append(tf.stack(fuzzified_values, axis = -1))\n",
    "\n",
    "        # stack everything and return:\n",
    "        return tf.stack(membership_values, axis = 1)\n",
    "\n",
    "# second layer -> firing strength layer:\n",
    "class FS_Layer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(FS_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, membership_values):\n",
    "        # this layer accepts the membership values, which have shape (batch_size, num_inputs, num_mfs):\n",
    "        batch_size = tf.shape(membership_values)[0]\n",
    "\n",
    "        # initialize the firing strengths:\n",
    "        firing_strengths = tf.ones((batch_size, self.num_rules), dtype = tf.float32)\n",
    "\n",
    "        # generate all the rule combinations:\n",
    "        rules = list(product(range(self.num_mfs), repeat = self.num_inputs))    # example [(0, 0, 0), (0, 0, 1), ...]\n",
    "\n",
    "        # need to check each input, each mf combination, and multiply their values together:\n",
    "        for rule_index, combination in enumerate(rules):\n",
    "            # print(f'combination: {combination}')\n",
    "            rule_strength = tf.ones((batch_size, ), dtype = tf.float32)\n",
    "\n",
    "            # for every input and membership function:\n",
    "            for input_index, mf_index in enumerate(combination):\n",
    "                # print(f'input: {input_index + 1} | mf: {mf_index + 1}')\n",
    "\n",
    "                # correctly extract the fuzzified values based on the combination index:\n",
    "                rule_strength *= membership_values[:, input_index, mf_index]\n",
    "            \n",
    "            # update the firing strengths:\n",
    "            rule_strength = tf.expand_dims(rule_strength, axis = -1)  # shape: (batch_size, 1)\n",
    "            firing_strengths = tf.concat(\n",
    "                [firing_strengths[:, :rule_index], rule_strength, firing_strengths[:, rule_index + 1:]],\n",
    "                axis = 1,\n",
    "            )\n",
    "            # print(f'firing strength: {firing_strengths}')\n",
    "\n",
    "        return firing_strengths\n",
    "    \n",
    "# third layer -> normalization layer:\n",
    "class NM_Layer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(NM_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, firing_strengths):\n",
    "        # this function accepts inputs of size (batch_size, num_rules).\n",
    "        # need to first get the total firing strength:\n",
    "        total_firing_strength = tf.reduce_sum(firing_strengths, axis = 1, keepdims = True)\n",
    "        \n",
    "        # can now normalize the firing strengths:\n",
    "        normalized_strengths = firing_strengths / (total_firing_strength + 1e-10)   # add a buffer in case the total firing strength is zero\n",
    "\n",
    "        return normalized_strengths\n",
    "    \n",
    "# fourth layer -> consequent layer:\n",
    "class CN_Layer(Layer):\n",
    "    # constructor: \n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(CN_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "        # need to initialize the consequent parameters:\n",
    "        self.consequent_params = self.add_weight(\n",
    "            shape = (self.num_rules, self.num_inputs + 1),\n",
    "            initializer = tf.keras.initializers.RandomUniform(-1.0, 1.0, seed = 1234),\n",
    "            trainable = True,\n",
    "            name = 'Consequent_Params'\n",
    "        )\n",
    "\n",
    "# add a spot where you can set the consequents!!!!!!!\n",
    "\n",
    "    # call function:\n",
    "    def call(self, input_list):\n",
    "        # unpack inputs from list:\n",
    "        normalized_strengths, inputs = input_list\n",
    "\n",
    "        # get the batch size:\n",
    "        batch_size = tf.shape(normalized_strengths)[0]\n",
    "\n",
    "        # the output is given by the multiplication of the inputs with the consequent weights,\n",
    "        # such as: o_k = w_bar_k * (x_1 * p_k + x_2 * q_k + x_3 * r_k + ... + s_k)\n",
    "        # can therefore extend the inputs to be (batch_size, num_inputs + bias) for ease of multiplication:\n",
    "        inputs_with_bias = tf.concat([inputs, tf.ones((batch_size, 1), dtype = tf.float32)], axis = -1)\n",
    "\n",
    "        # need to now reshape the normalized strengths to be of size (batch_size, num_rules, 1)\n",
    "        # this effectively flips it into a 'column vector' of sorts, where each individual value is now vertically aligned\n",
    "        normalized_strengths = tf.reshape(normalized_strengths, (batch_size, self.num_rules, 1))\n",
    "\n",
    "        # get the consequent parameters, which have shape (num_rules, num_inputs + 1):\n",
    "        consequent_params = self.consequent_params\n",
    "\n",
    "        # expand inputs with bias to match the rule axis: (batch_size, num_rules, num_inputs + 1)\n",
    "        inputs_with_bias_expanded = tf.expand_dims(inputs_with_bias, axis = 1)\n",
    "\n",
    "        # calculate the consequent for each rule\n",
    "        consequents = tf.reduce_sum(normalized_strengths * inputs_with_bias_expanded * consequent_params, axis = 2)\n",
    "\n",
    "        return consequents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "values for making model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the testing parameters that I am using for the membership functions:\n",
    "params = tf.constant(np.array([\n",
    "    [  # Parameters for input 1\n",
    "        [0.0, 0.0, 6.0],\n",
    "        [5/6, 5.0, 55/6],\n",
    "        [4.0, 10.0, 10.0]\n",
    "    ],\n",
    "    [  # Parameters for input 2\n",
    "        [0.0, 0.0 , 15.0],\n",
    "        [25/12, 12.5, 275/12],\n",
    "        [10.0, 25.0, 25.0]\n",
    "    ],\n",
    "    [  # Parameters for input 3\n",
    "        [0.0, 0.0, 30.0],\n",
    "        [25/6, 25.0, 275/6],\n",
    "        [15.0, 50.0, 50.0]\n",
    "    ]\n",
    "]), dtype = tf.float32)\n",
    "\n",
    "input = tf.constant([[8, 5, 32], [2, 17, 22]], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 output: \n",
      "[[[0.         0.28000006 0.6666667 ]\n",
      "  [0.6666667  0.28       0.        ]\n",
      "  [0.         0.664      0.4857143 ]]\n",
      "\n",
      " [[0.6666667  0.28000003 0.        ]\n",
      "  [0.         0.56799996 0.46666667]\n",
      "  [0.26666668 0.856      0.2       ]]]\n",
      "\n",
      "layer 2 output: \n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.12394669 0.09066669\n",
      "  0.         0.05205761 0.03808001 0.         0.         0.\n",
      "  0.         0.29511112 0.21587303 0.         0.12394666 0.09066667\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.10097778 0.32413864 0.07573333\n",
      "  0.08296297 0.2663111  0.06222222 0.         0.         0.\n",
      "  0.04241067 0.13613825 0.031808   0.03484445 0.11185069 0.02613334\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "\n",
      "layer 3 output: \n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.12029588 0.08799613\n",
      "  0.         0.05052427 0.03695837 0.         0.         0.\n",
      "  0.         0.28641874 0.20951457 0.         0.12029585 0.08799611\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.07794313 0.2501974  0.05845734\n",
      "  0.06403778 0.20556128 0.04802833 0.         0.         0.\n",
      "  0.03273612 0.10508293 0.02455208 0.02689587 0.08633575 0.0201719\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "\n",
      "layer 4 output: \n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          2.8482838  -0.28457433\n",
      "   0.         -0.20993418  1.0630933   0.          0.          0.\n",
      "   0.         -3.6986122   1.272747    0.          3.2848604  -0.78844\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.44080722  3.4329615   1.1179184\n",
      "   0.6405227   3.7094097   0.81404734  0.          0.          0.\n",
      "   0.54280126 -0.15962103  0.5484029  -0.34405598 -1.9327437  -0.6083515\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test first layer:\n",
    "mf = MF_Layer(3, 3)     # instantiate\n",
    "mf.set_weights(params)  # set to testing parameters\n",
    "\n",
    "# pass values through first layer:\n",
    "mf_output = mf(input)       # pass values\n",
    "print(f'layer 1 output: \\n{mf_output}\\n')\n",
    "\n",
    "# pass values through second layer:\n",
    "fs = FS_Layer(3, 3)         # instantiate\n",
    "fs_output = fs(mf_output)   # pass values\n",
    "print(f'layer 2 output: \\n{fs_output}\\n')\n",
    "\n",
    "# pass values through third layer:\n",
    "nm = NM_Layer(3, 3)         # instantiate\n",
    "nm_output = nm(fs_output)   # pass values\n",
    "print(f'layer 3 output: \\n{nm_output}\\n')\n",
    "\n",
    "# pass values through the fourth layer:\n",
    "cn = CN_Layer(3, 3)                 # instantiate\n",
    "cn_output = cn([nm_output, input])  # pass values\n",
    "print(f'layer 4 output: \\n{cn_output}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
