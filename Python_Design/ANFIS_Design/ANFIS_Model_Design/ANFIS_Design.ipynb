{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "\n",
    "This file serves to host an ANFIS model created for use in multi-robot task allocation (MRTA). This model performs regression based on the load history of a robot, its distance to the current task, as well as the total distance that it has travelled thus far. The goal for designing this ANFIS is to compare its performance against an ANN to determine which is better at approximating an FIS for task allocation. This will be achieved through comparing metrics such as the coefficient of determination ($R^{2}$), root mean squared error (RMSE), and mean absolute error (MAE).\n",
    "\n",
    "**Date Created: 4/2/2025**\n",
    "\n",
    "**Date Modified: 4/2/2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages:**\n",
    "\n",
    "This section imports all necessary packages for the ANN implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model, constraints\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Layer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define Model Layers:**\n",
    "\n",
    "This section defines the layers that are used within the model as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to define a constraint for training the parameters:\n",
    "class OrderedConstraint(constraints.Constraint):\n",
    "    # constructor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # call function for constraint:\n",
    "    def __call__(self, W):\n",
    "        return tf.sort(W, axis = 2)\n",
    "\n",
    "# first layer -> membership layer:\n",
    "class MF_Layer(Layer): \n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, mf_type, **kwargs):\n",
    "        super(MF_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "\n",
    "        # check if string passed:\n",
    "        if not type(mf_type) is str:\n",
    "            raise TypeError('Only strings are permitted to be passed')\n",
    "        \n",
    "        # check if a recognized membership function was passed:\n",
    "        if any(mf in mf_type for mf in ['Smoothed Triangular', 'Gaussian', 'Generalized Bell']):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Unrecognized MF passed to function')\n",
    "        \n",
    "        # assign mf type to object, which will determine the number of parameters generated:\n",
    "        if mf_type == 'Gaussian':\n",
    "            self.mf_type = 'Gaussian'\n",
    "            self.num_antecedents = 2\n",
    "            self.constraints = None\n",
    "            self.init_max = 50.0\n",
    "            self.init_min = 0.0\n",
    "        elif mf_type == 'Smoothed Triangular':\n",
    "            self.mf_type = 'Smoothed Triangular'\n",
    "            self.num_antecedents = 3\n",
    "            self.constraints = OrderedConstraint()\n",
    "            self.init_max = 50.0\n",
    "            self.init_min = 0.0\n",
    "        elif mf_type == 'Generalized Bell':\n",
    "            self.mf_type = 'Generalized Bell'\n",
    "            self.num_antecedents = 3\n",
    "            self.constraints = None\n",
    "            self.init_max = 50.0\n",
    "            self.init_min = 1.0\n",
    "\n",
    "        # need to initialize antecedent parameters\n",
    "        self.mf_params = self.add_weight(\n",
    "            shape = (self.num_inputs, self.num_mfs, self.num_antecedents),             \n",
    "            initializer= tf.keras.initializers.RandomUniform(self.init_min, self.init_max),\n",
    "            trainable = True,\n",
    "            name = 'Antecedent_Params',\n",
    "            constraint = self.constraints\n",
    "        )\n",
    "\n",
    "    # custom setting of weights:\n",
    "    def set_weights(self, params):\n",
    "        # this function is used to set weights based on what a user provides\n",
    "        # user must provide weights in the form of a np.array of shape (num_mfs, num_params)\n",
    "\n",
    "        if params.shape != (self.num_inputs, self.num_mfs, self.num_antecedents):\n",
    "            raise ValueError(f'Parameters provided are not of correct shape, expected ({self.num_inputs}, {self.num_mfs}, {self.num_antecedents})')\n",
    "\n",
    "        self.mf_params = params\n",
    "\n",
    "    # function call:\n",
    "    def call(self, inputs):\n",
    "        # need to initialize the membership values:\n",
    "        membership_values = []\n",
    "\n",
    "        # for every input:\n",
    "        for i in range(self.num_inputs):\n",
    "            # get the memberships for that input:\n",
    "            input_mf_params = self.mf_params[i]\n",
    "\n",
    "            # need to now compute the fuzzified value for each membership function:\n",
    "            fuzzified_values = []\n",
    "\n",
    "            # for every membership function:\n",
    "            for j in range(self.num_mfs):\n",
    "\n",
    "                # if gaussian:\n",
    "                if self.mf_type == 'Gaussian':\n",
    "                    # define parameters:\n",
    "                    mean = input_mf_params[j, 0]  # mean of the gaussian\n",
    "                    std = input_mf_params[j, 1]   # standard deviation of the gaussian\n",
    "\n",
    "                    # compute output:\n",
    "                    output = tf.exp(-0.5 * tf.square((inputs[:, i] - mean) / (std + 1e-6)))\n",
    "                    fuzzified_values.append(output)\n",
    "\n",
    "                # if smoothed triangular:\n",
    "                if self.mf_type == 'Smoothed Triangular':\n",
    "                    # define parameters\n",
    "                    a = input_mf_params[j, 0]   # a parameter\n",
    "                    b = input_mf_params[j, 1]   # b parameter\n",
    "                    c = input_mf_params[j, 2]   # c parameter\n",
    "\n",
    "                    # smoothing factor beta:\n",
    "                    beta = 100.0\n",
    "\n",
    "                    # check if we are on the edges:\n",
    "                    is_left_edge = tf.equal(a, b)\n",
    "                    is_right_edge = tf.equal(b, c)\n",
    "\n",
    "                    # compute softplus-based smoothed triangular membership function:\n",
    "                    left = tf.nn.softplus(beta * (inputs[:, i] - a)) / (tf.nn.softplus(beta * (b - a)) + 1e-6)\n",
    "                    right = tf.nn.softplus(beta * (c - inputs[:, i])) / (tf.nn.softplus(beta * (c - b)) + 1e-6)\n",
    "\n",
    "                    # deal with edge case:\n",
    "                    left = tf.where((inputs[:, i] == a) & is_left_edge, 1.0, left)\n",
    "                    right = tf.where((inputs[:, i] == c) & is_right_edge, 1.0, right)\n",
    "\n",
    "                    # compute output:\n",
    "                    output = tf.maximum(0.0, tf.minimum(left, right))\n",
    "                    fuzzified_values.append(output)\n",
    "\n",
    "                # if generalized bell:\n",
    "                if self.mf_type == 'Generalized Bell':\n",
    "                    # define parameters\n",
    "                    a = input_mf_params[j, 0]\n",
    "                    b = input_mf_params[j, 1]\n",
    "                    c = input_mf_params[j, 2]\n",
    "\n",
    "                    # clamp b:\n",
    "                    b = tf.clip_by_value(b, 1e-6, 5.0)\n",
    "\n",
    "                    # compute output:\n",
    "                    output = 1 / (1 + tf.abs((inputs[:, i] - c) / (a + 1e-6)) ** (2 * b))\n",
    "                    fuzzified_values.append(output)\n",
    "            \n",
    "            # need to now stack the mf values for that given input:\n",
    "            membership_values.append(tf.stack(fuzzified_values, axis = -1))\n",
    "\n",
    "        # stack everything and return:\n",
    "        return tf.stack(membership_values, axis = 1)\n",
    "    \n",
    "# second layer -> firing strength layer:\n",
    "class FS_Layer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(FS_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, membership_values):\n",
    "        # this layer accepts the membership values, which have shape (batch_size, num_inputs, num_mfs):\n",
    "        batch_size = tf.shape(membership_values)[0]\n",
    "\n",
    "        # initialize the firing strengths:\n",
    "        firing_strengths = tf.ones((batch_size, self.num_rules), dtype = tf.float32)\n",
    "\n",
    "        # generate all the rule combinations:\n",
    "        rules = list(product(range(self.num_mfs), repeat = self.num_inputs))    # example [(0, 0, 0), (0, 0, 1), ...]\n",
    "\n",
    "        # need to check each input, each mf combination, and multiply their values together:\n",
    "        for rule_index, combination in enumerate(rules):\n",
    "            # print(f'combination: {combination}')\n",
    "            rule_strength = tf.ones((batch_size, ), dtype = tf.float32)\n",
    "\n",
    "            # for every input and membership function:\n",
    "            for input_index, mf_index in enumerate(combination):\n",
    "                # print(f'input: {input_index + 1} | mf: {mf_index + 1}')\n",
    "\n",
    "                # correctly extract the fuzzified values based on the combination index:\n",
    "                rule_strength *= membership_values[:, input_index, mf_index] + 1e-6\n",
    "            \n",
    "            # update the firing strengths:\n",
    "            rule_strength = tf.expand_dims(rule_strength, axis = -1)  # shape: (batch_size, 1)\n",
    "            firing_strengths = tf.concat(\n",
    "                [firing_strengths[:, :rule_index], rule_strength, firing_strengths[:, rule_index + 1:]],\n",
    "                axis = 1,\n",
    "            )\n",
    "            # print(f'firing strength: {firing_strengths}')\n",
    "\n",
    "        return firing_strengths\n",
    "    \n",
    "# third layer -> normalization layer:\n",
    "class NM_Layer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(NM_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, firing_strengths):\n",
    "        # this function accepts inputs of size (batch_size, num_rules).\n",
    "        # need to first get the total firing strength:\n",
    "        total_firing_strength = tf.reduce_sum(firing_strengths, axis = 1, keepdims = True)\n",
    "        \n",
    "        # can now normalize the firing strengths:\n",
    "        normalized_strengths = firing_strengths / (total_firing_strength + 1e-10)   # add a buffer in case the total firing strength is zero\n",
    "\n",
    "        return normalized_strengths\n",
    "    \n",
    "# fourth layer -> consequent layer:\n",
    "class CN_Layer(Layer):\n",
    "    # constructor: \n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(CN_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "        # need to initialize the consequent parameters:\n",
    "        self.consequent_params = self.add_weight(\n",
    "            shape = (self.num_rules, self.num_inputs + 1),\n",
    "            initializer = tf.keras.initializers.RandomUniform(-1.0, 1.0, seed = 1234),\n",
    "            trainable = True,\n",
    "            name = 'Consequent_Params'\n",
    "        )\n",
    "\n",
    "    # this function is used for manually setting the consequent parameters:\n",
    "    def set_cons(self, params):\n",
    "        # this function accepts parameters as an array of size (num_rules, num_inputs + 1):\n",
    "        if params.shape != (self.num_rules, self.num_inputs + 1):\n",
    "            raise ValueError(f'Parameters provided are not of correct shape, expected ({self.num_rules}, {self.num_inputs + 1})')\n",
    "        \n",
    "        # assign parameters:\n",
    "        self.consequent_params = params\n",
    "\n",
    "    # call function:\n",
    "    def call(self, input_list):\n",
    "        # unpack inputs from list:\n",
    "        normalized_strengths, inputs = input_list\n",
    "\n",
    "        # get the batch size:\n",
    "        batch_size = tf.shape(normalized_strengths)[0]\n",
    "\n",
    "        # the output is given by the multiplication of the inputs with the consequent weights,\n",
    "        # such as: o_k = w_bar_k * (x_1 * p_k + x_2 * q_k + x_3 * r_k + ... + s_k)\n",
    "        # can therefore extend the inputs to be (batch_size, num_inputs + bias) for ease of multiplication:\n",
    "        inputs_with_bias = tf.concat([inputs, tf.ones((batch_size, 1), dtype = tf.float32)], axis = -1)\n",
    "\n",
    "        # need to now reshape the normalized strengths to be of size (batch_size, num_rules, 1)\n",
    "        # this effectively flips it into a 'column vector' of sorts, where each individual value is now vertically aligned\n",
    "        normalized_strengths = tf.reshape(normalized_strengths, (batch_size, self.num_rules, 1))\n",
    "\n",
    "        # get the consequent parameters, which have shape (num_rules, num_inputs + 1):\n",
    "        consequent_params = self.consequent_params\n",
    "\n",
    "        # expand inputs with bias to match the rule axis: (batch_size, num_rules, num_inputs + 1)\n",
    "        inputs_with_bias_expanded = tf.expand_dims(inputs_with_bias, axis = 1)\n",
    "\n",
    "        # calculate the consequent for each rule\n",
    "        consequents = tf.reduce_sum(normalized_strengths * inputs_with_bias_expanded * consequent_params, axis = 2)\n",
    "\n",
    "        return consequents\n",
    "\n",
    "# fifth layer -> output layer:\n",
    "class O_Layer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(O_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_output = num_mfs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, consequents):\n",
    "        output = tf.reduce_sum(consequents, axis = 1, keepdims = True)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Importation:**\n",
    "\n",
    "This section imports and processes the data for use in the ANFIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "# import data from csv as pandas dataframe:\n",
    "data = pd.read_csv('V3_Data.csv')\n",
    "print('\\nData loaded sucessfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into X and Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform split:\n",
    "x_data = data.drop(columns = 'Suitability').astype('float32').values\n",
    "y_data = data['Suitability'].astype('float32').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train, validation, and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples have shape: (8000, 3)\n",
      "Validation examples have shape: (1000, 3)\n",
      "Testing examples have shape:(1000, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the data using train_test_split:\n",
    "x_train, x_filler, y_train, y_filler = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_filler, y_filler, test_size = 0.5)\n",
    "\n",
    "# get the split results:\n",
    "print(f'Training examples have shape: {x_train.shape}')\n",
    "print(f'Validation examples have shape: {x_val.shape}')\n",
    "print(f'Testing examples have shape:{x_test.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a scaler:\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale each set:\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# save the scaler:\n",
    "dump(scaler, open('scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create the Model:**\n",
    "\n",
    "This section defines the relevant metrics and loss functions, and then defines a function for creating models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FUNCTION = 'mse'\n",
    "METRICS = ['mae', tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.R2Score()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model generation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function:\n",
    "def BuildAnfis(input_shape, num_inputs, num_mfs, mf_type, rate):\n",
    "    # define the inputs:\n",
    "    inputs = Input(shape = input_shape)\n",
    "\n",
    "    # add the custom layers:\n",
    "    membership_layer = MF_Layer(num_inputs = num_inputs, num_mfs = num_mfs, mf_type = mf_type)(inputs)\n",
    "    firing_layer = FS_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(membership_layer)\n",
    "    normalization_layer = NM_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(firing_layer)\n",
    "    consequent_layer = CN_Layer(num_inputs = num_inputs, num_mfs = num_mfs)([normalization_layer, inputs])\n",
    "    output_layer = O_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(consequent_layer)\n",
    "\n",
    "    # compile the model:\n",
    "    model = Model(inputs = inputs, outputs = output_layer)\n",
    "    model.compile(optimizer = Adam(learning_rate = rate), \n",
    "                  loss = LOSS_FUNCTION, \n",
    "                  metrics = METRICS)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model:\n",
    "tf.keras.backend.clear_session()\n",
    "model = BuildAnfis(input_shape = (3, ),\n",
    "                    num_inputs = 3,\n",
    "                    num_mfs = 5,\n",
    "                    mf_type = 'Generalized Bell',\n",
    "                    rate = 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train the Model:**\n",
    "\n",
    "This section trains the model using the data that was previously split."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
