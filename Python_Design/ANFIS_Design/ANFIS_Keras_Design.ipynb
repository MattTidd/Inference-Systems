{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "\n",
    "This file serves to host an attempted Keras implementation of an Adaptive Neuro-Fuzzy Inference System (ANFIS).\n",
    "\n",
    "**Date Created:** 22/01/2025\n",
    "\n",
    "**Date Modified:** 28/01/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages:**\n",
    "\n",
    "This section imports all the necessary packages for the ANFIS implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers, constraints\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from pickle import dump\n",
    "from itertools import product\n",
    "from keras.layers import Layer, Dense, Reshape, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Input, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function & Layer Definition:**\n",
    "\n",
    "This section creates the necessary custom functions and layers for this ANFIS implementation within Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to first define the initial layer -> the membership function layer:\n",
    "class MembershipFunctionLayer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, params = None, **kwargs):   # by including **kwargs, we allow for additional arguments from keras, like name or dtype\n",
    "        super(MembershipFunctionLayer, self).__init__(**kwargs)         # we are subclassing from the keras layer -> telling the constructor to make our layer like a keras layer\n",
    "        self.num_inputs = num_inputs            # define the number of inputs to the ANFIS \n",
    "        self.num_mfs = num_mfs                  # define the number of membership functions per input\n",
    "        self.num_rules = num_mfs ** num_inputs  # the number of rules is calculated as such:\n",
    "\n",
    "        # next is the initialization of the antecedent parameters:\n",
    "        if params is not None:\n",
    "            # initialize custom parameters defined by the user:\n",
    "            self.mf_params = self.add_weight(\n",
    "                shape=(self.num_inputs, self.num_mfs, 3),       # define their shape, (num_inputs, num_mfs, 3) as we have 3 params for a triangular mf\n",
    "                initializer=tf.constant_initializer(params),    # initialize as constants from the provided array\n",
    "                trainable=True,                                 # set to trainable\n",
    "                name=\"Antecedent Params\",                       # assign them a name\n",
    "            )\n",
    "            print('Custom parameters have been set.')\n",
    "        else:\n",
    "            # initialize raw membership parameters:\n",
    "            raw_params = self.add_weight(\n",
    "                shape = (self.num_inputs, self.num_mfs, 3),                                 # define their shape, (num_inputs, num_mfs, 3) \n",
    "                initializer = tf.keras.initializers.RandomUniform(-1.0, 1.0, seed = 1234),  # initialize as a random, uniform distribution\n",
    "                trainable = True,                                                           # set to trainable\n",
    "                name = \"Raw Antecedent Params\",                                             # assign them a name \n",
    "            )\n",
    "\n",
    "            # sort the parameters such that a <= b <= c:\n",
    "            sorted_params = tf.sort(raw_params, axis = 1)           # sort such that a <= b <= c\n",
    "            self.mf_params = self.add_weight(                       \n",
    "                shape = (self.num_inputs, self.num_mfs, 3),                     # set the shape: (num_inputs, num_mfs, 3) as we have 3 params for a triangular mf\n",
    "                initializer = tf.constant_initializer(sorted_params.numpy()),   # initialize as the sorted array of params\n",
    "                trainable = True,                                               # set to trainable\n",
    "                name = 'Antecedent Params'                                      # assign them a name\n",
    "            )\n",
    "            print('Random parameters have been set.')\n",
    " \n",
    "    # function for visualizing the membership functions:\n",
    "    def plot_mf(self, max_values, mf_names = None):\n",
    "        # if the user did not provide names:\n",
    "        if mf_names is None:\n",
    "            mf_names = [f'MF {i + 1}' for i in range(self.num_mfs)]\n",
    "        \n",
    "        # make sure that the number of names matches the number of membership functions:\n",
    "        if len(mf_names) != self.num_mfs:\n",
    "            raise ValueError(f'Expected {self.num_mfs} membership functions, but got {len(mf_names)} instead.')\n",
    "        \n",
    "        # make sure that the provided max values match the number of membership functions:\n",
    "        if len(max_values) != self.num_mfs:\n",
    "            raise ValueError(f'Expected {self.num_mfs} max values, but got {len(max_values)} instead.') \n",
    "        \n",
    "        # create linspace based on max values:\n",
    "        input_range = {}\n",
    "        for i in range(self.num_inputs):\n",
    "            input_range[i] = np.linspace(0, max_values[i], 1000)\n",
    "\n",
    "        # plot the mfs:\n",
    "        for input_index in range(self.num_inputs):\n",
    "            x_values = input_range[input_index]\n",
    "            plt.figure(figsize = (12,8))\n",
    "\n",
    "            # plot each mf for the selected input:\n",
    "            for i in range(self.num_mfs):\n",
    "                params = self.mf_params[input_index, i].numpy()\n",
    "                a, b, c = params\n",
    "                y_values = [np.maximum(0.0, np.minimum((x - a) / (b - a + 1e-6), (c - x) / (c - b + 1e-6))) for x in x_values]\n",
    "                \n",
    "                plt.plot(x_values, y_values, label = f'{mf_names[i]}')\n",
    "            plt.title(f'Membership Functions for Input X{input_index + 1}')\n",
    "            plt.xlabel('Input Value')\n",
    "            plt.ylabel('Degree of Membership')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    # need to define the call -> this is what gets executed by the layer:\n",
    "    def call(self, inputs):\n",
    "        # initialize list to hold membership values:\n",
    "        membership_values = []\n",
    "\n",
    "        # loop through each input:\n",
    "        for i in range(self.num_inputs):\n",
    "            # extract membership functions for the current input:\n",
    "            input_mf_params = self.mf_params[i]  # symbolic tensor\n",
    "\n",
    "            # compute membership values for all MFs for the current input:\n",
    "            mf_values = []\n",
    "\n",
    "            for j in range(self.num_mfs):\n",
    "                a = input_mf_params[j, 0]\n",
    "                b = input_mf_params[j, 1]\n",
    "                c = input_mf_params[j, 2]\n",
    "\n",
    "                mf_value = tf.maximum(\n",
    "                    0.0,\n",
    "                    tf.minimum(\n",
    "                        (inputs[:, i] - a) / (b - a + 1e-6),\n",
    "                        (c - inputs[:, i]) / (c - b + 1e-6),\n",
    "                    ),\n",
    "                )\n",
    "                mf_values.append(mf_value)\n",
    "\n",
    "            # stack MFs for this input\n",
    "            membership_values.append(tf.stack(mf_values, axis = -1))\n",
    "\n",
    "        # stack all membership values (shape: batch_size, num_inputs, num_mfs)\n",
    "        return tf.stack(membership_values, axis = 1)\n",
    "\n",
    "# need to now define the second layer -> the firing strength layer:\n",
    "class FiringStrengthLayer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(FiringStrengthLayer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, membership_values):\n",
    "        # get batch size:\n",
    "        batch_size = tf.shape(membership_values)[0]  \n",
    "\n",
    "        # initialize firing strengths\n",
    "        firing_strengths = tf.ones((batch_size, self.num_rules), dtype = tf.float32)    \n",
    "\n",
    "        # generate all rule combinations:\n",
    "        rules = list(product(range(self.num_mfs), repeat = self.num_inputs))  # example: [(0, 0, 0), (0, 0, 1), ...]\n",
    "\n",
    "        # need to check each input, each mf combination, and multiply their values together:\n",
    "        for rule_index, combination in enumerate(rules):\n",
    "            # print(f'rule: {rule_index + 1} | combination: {combination}')\n",
    "            rule_strength = tf.ones((batch_size, ), dtype = tf.float32)\n",
    "\n",
    "            for input_index, mf_index in enumerate(combination):\n",
    "                # print(f'input: {input_index + 1} | mf: {mf_index + 1}')\n",
    "\n",
    "                # correctly extract the fuzzified values based on the combination index:\n",
    "                rule_strength *= membership_values[:, input_index, mf_index]\n",
    "                # print(f'strength: {rule_strength}')\n",
    "            \n",
    "            # update the firing strengths:\n",
    "            rule_strength = tf.expand_dims(rule_strength, axis = -1)  # shape: (batch_size, 1)\n",
    "            firing_strengths = tf.concat(\n",
    "                [firing_strengths[:, :rule_index], rule_strength, firing_strengths[:, rule_index + 1:]],\n",
    "                axis = 1,\n",
    "            )\n",
    "\n",
    "        return firing_strengths\n",
    "\n",
    "# need to now define the third layer -> the normalization layer:\n",
    "class NormalizationLayer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(NormalizationLayer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, firing_strengths):\n",
    "        # get batch size:\n",
    "        batch_size = tf.shape(firing_strengths)[0]\n",
    "\n",
    "        # get total firing strength:\n",
    "        total_strength = tf.reduce_sum(firing_strengths, axis = 1, keepdims = True)\n",
    "        # print(f'total strength: {total_strength}')\n",
    "        \n",
    "        # normalize the firing strengths:\n",
    "        normalized_strengths = firing_strengths / (total_strength + 1e-10)\n",
    "\n",
    "        return normalized_strengths\n",
    "\n",
    "# need to now define the fourth layer -> the consequent layer:\n",
    "class ConsequentLayer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(ConsequentLayer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "        # need to also initialize the consequent parameters:\n",
    "        self.consequent_params = self.add_weight(\n",
    "            shape = (self.num_rules, self.num_inputs + 1),\n",
    "            initializer = tf.keras.initializers.RandomUniform(-1.0, 1.0, seed = 1234),\n",
    "            trainable = True,\n",
    "            name = 'Consequent Params'\n",
    "        )\n",
    "\n",
    "    # call function:\n",
    "    def call(self, input_list):\n",
    "        # inputs are a list -> extract values:\n",
    "        normalized_strengths, inputs = input_list\n",
    "\n",
    "        # get the batch size\n",
    "        batch_size = tf.shape(normalized_strengths)[0]\n",
    "\n",
    "        # add bias term to inputs: shape (batch_size, num_inputs + 1)\n",
    "        inputs_with_bias = tf.concat([inputs, tf.ones((batch_size, 1), dtype=tf.float32)], axis = -1)\n",
    "\n",
    "        # reshape normalized_strengths to (batch_size, num_rules, 1)\n",
    "        normalized_strengths = tf.reshape(normalized_strengths, (batch_size, self.num_rules, 1))\n",
    "\n",
    "        # get consequent parameters: shape (num_rules, num_inputs + 1)\n",
    "        consequent_params = self.consequent_params  # already initialized as a weight\n",
    "\n",
    "        # expand inputs_with_bias to match the rule axis: (batch_size, num_rules, num_inputs + 1)\n",
    "        inputs_with_bias_expanded = tf.expand_dims(inputs_with_bias, axis = 1)\n",
    "\n",
    "        # calculate the consequent for each rule\n",
    "        consequents = tf.reduce_sum(normalized_strengths * inputs_with_bias_expanded * consequent_params, axis = 2)\n",
    "\n",
    "        return consequents\n",
    "\n",
    "# now can define the final layer -> the output layer:\n",
    "class OutputLayer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(OutputLayer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, consequents):\n",
    "        output = tf.reduce_sum(consequents, axis = 1, keepdims = True)\n",
    "        return output\n",
    "\n",
    "# define a custom function for building models:\n",
    "def BuildAnfis(input_shape, num_inputs, num_mfs, antecedent_params = None):\n",
    "    # define the inputs:\n",
    "    inputs = Input(shape = input_shape)\n",
    "\n",
    "    # add the custom layers:\n",
    "    membership_layer = MembershipFunctionLayer(num_inputs = num_inputs, num_mfs = num_mfs, params = antecedent_params)(inputs)\n",
    "    firing_layer = FiringStrengthLayer(num_inputs = num_inputs, num_mfs = num_mfs)(membership_layer)\n",
    "    normalization_layer = NormalizationLayer(num_inputs = num_inputs, num_mfs = num_mfs)(firing_layer)\n",
    "    consequent_layer = ConsequentLayer(num_inputs = num_inputs, num_mfs = num_mfs)([normalization_layer, inputs])\n",
    "    output_layer = OutputLayer(num_inputs = num_inputs, num_mfs = num_mfs)(consequent_layer)\n",
    "\n",
    "    # create and compile the model:\n",
    "    model = Model(inputs = inputs, outputs = output_layer)\n",
    "    model.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Import & Processing:**\n",
    "\n",
    "This section imports the required data and splits it into training, validation, and testing sets. From this, the model can be trained and its performance verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# get the data path:\n",
    "files_in_dir = os.listdir(os.getcwd())\n",
    "data_path = os.path.join(os.getcwd(), files_in_dir[files_in_dir.index('V3_Data.csv')])\n",
    "\n",
    "# load data based on data path:\n",
    "df = pd.read_csv(data_path)\n",
    "print('Data successfully loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to first split into features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and Y:\n",
    "x_data = df.drop(columns = 'Suitability')\n",
    "y_data = df['Suitability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data can be split into training, validation, and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8000 training examples\n",
      "There are 1000 validation examples\n",
      "There are 1000 testing examples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the data using train_test_split:\n",
    "x_train, x_filler, y_train, y_filler = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_filler, y_filler, test_size = 0.5)\n",
    "\n",
    "# get the split results:\n",
    "print(f'There are {x_train.shape[0]} training examples')\n",
    "print(f'There are {x_val.shape[0]} validation examples')\n",
    "print(f'There are {x_test.shape[0]} testing examples\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can scale the data. The data must be scaled using the same scaler used on the training set, to ensure that the model is consistent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define a scaler:\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # scale each set:\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_val = scaler.transform(x_val)\n",
    "# x_test = scaler.transform(x_test)\n",
    "\n",
    "# # save the scaler:\n",
    "# dump(scaler, open('scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Parameter Definition**\n",
    "\n",
    "This section define the parameters used in generating a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the following to be used in model generation:\n",
    "num_inputs = 3\n",
    "num_mfs = 3\n",
    "max_values = np.array([10, 25, 50])\n",
    "mf_names = ['Low', 'Medium', 'High']\n",
    "params = np.array([\n",
    "    [  # Parameters for input 1\n",
    "        [0, 0, 6],\n",
    "        [5/6, 5, 55/6],\n",
    "        [4, 10, 10]\n",
    "    ],\n",
    "    [  # Parameters for input 2\n",
    "        [0, 0 , 15],\n",
    "        [25/12, 12.5, 275/12],\n",
    "        [10, 25, 25]\n",
    "    ],\n",
    "    [  # Parameters for input 3\n",
    "        [0, 0, 30],\n",
    "        [25/6, 25, 275/6],\n",
    "        [15, 50, 50]\n",
    "    ]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "going to test each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom parameters have been set.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling MembershipFunctionLayer.call().\n\n\u001b[1m{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: membership_function_layer_14/strided_slice/\u001b[0m\n\nArguments received by MembershipFunctionLayer.call():\n  • inputs=tf.Tensor(shape=(3,), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# inputs = tf.constant([[2, 9, 21], [8, 23, 48]], dtype=tf.float32)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m mf_layer \u001b[38;5;241m=\u001b[39m MembershipFunctionLayer(num_inputs \u001b[38;5;241m=\u001b[39m num_inputs, num_mfs \u001b[38;5;241m=\u001b[39m num_mfs, params \u001b[38;5;241m=\u001b[39m params)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmf_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mtidd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[123], line 98\u001b[0m, in \u001b[0;36mMembershipFunctionLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     92\u001b[0m     b \u001b[38;5;241m=\u001b[39m input_mf_params[j, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     93\u001b[0m     c \u001b[38;5;241m=\u001b[39m input_mf_params[j, \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     95\u001b[0m     mf_value \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmaximum(\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     97\u001b[0m         tf\u001b[38;5;241m.\u001b[39mminimum(\n\u001b[1;32m---> 98\u001b[0m             (\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m a) \u001b[38;5;241m/\u001b[39m (b \u001b[38;5;241m-\u001b[39m a \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m),\n\u001b[0;32m     99\u001b[0m             (c \u001b[38;5;241m-\u001b[39m inputs[:, i]) \u001b[38;5;241m/\u001b[39m (c \u001b[38;5;241m-\u001b[39m b \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m),\n\u001b[0;32m    100\u001b[0m         ),\n\u001b[0;32m    101\u001b[0m     )\n\u001b[0;32m    102\u001b[0m     mf_values\u001b[38;5;241m.\u001b[39mappend(mf_value)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# stack MFs for this input\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling MembershipFunctionLayer.call().\n\n\u001b[1m{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: membership_function_layer_14/strided_slice/\u001b[0m\n\nArguments received by MembershipFunctionLayer.call():\n  • inputs=tf.Tensor(shape=(3,), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# get the input:\n",
    "value = np.array(x_train)[0]\n",
    "input = tf.constant(value, dtype = tf.float32)\n",
    "print(input)\n",
    "inputs = tf.constant([2, 9, 21], dtype=tf.float32)\n",
    "print(inputs)\n",
    "\n",
    "\n",
    "\n",
    "# mf_layer = MembershipFunctionLayer(num_inputs = num_inputs, num_mfs = num_mfs, params = params)\n",
    "# mf_layer(input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
