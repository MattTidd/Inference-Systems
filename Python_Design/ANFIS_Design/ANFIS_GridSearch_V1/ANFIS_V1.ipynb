{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "\n",
    "This file serves to perform a grid search to determine a baseline for the best hyperparameters for an ANFIS model for use in multi-robot task allocation through regression on FIS-generated data. The goal for designing this ANFIS is to compare its performance against an ANFIS to determine which is better at approximating the FIS, which will be achieved through the use of the coefficient of determination $R^{2}$, root mean square error (RMSE), and mean absolute error (MAE).\n",
    "\n",
    "**Date Created: 3/2/2025**\n",
    "\n",
    "**Date Modified: 3/2/2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages:**\n",
    "\n",
    "This section imports all necessary packages for the ANFIS implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import time \n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model, constraints\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define Model Class:**\n",
    "\n",
    "This section defines the classes that make up the constituent layers of the ANFIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomizer seed:\n",
    "np.random.seed(0)\n",
    "\n",
    "# need to define a constraint for training the parameters:\n",
    "class OrderedConstraint(constraints.Constraint):\n",
    "    # constructor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # call function for constraint:\n",
    "    def __call__(self, W):\n",
    "        return tf.sort(W, axis = 2)\n",
    "\n",
    "# first layer -> membership layer:\n",
    "class MF_Layer(Layer): \n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, mf_type, **kwargs):\n",
    "        super(MF_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "\n",
    "        # check if string passed:\n",
    "        if not type(mf_type) is str:\n",
    "            raise TypeError('Only strings are permitted to be passed')\n",
    "        \n",
    "        # check if a recognized membership function was passed:\n",
    "        if any(mf in mf_type for mf in ['Smoothed Triangular', 'Gaussian', 'Generalized Bell']):\n",
    "            print('Accepted Membership Function passed')\n",
    "        else:\n",
    "            raise ValueError('Unrecognized MF passed to function')\n",
    "        \n",
    "        # assign mf type to object, which will determine the number of parameters generated:\n",
    "        if mf_type == 'Gaussian':\n",
    "            self.mf_type = 'Gaussian'\n",
    "            self.num_antecedents = 2\n",
    "            self.constraints = None\n",
    "            self.init_max = 50.0\n",
    "            self.init_min = 0.0\n",
    "        elif mf_type == 'Smoothed Triangular':\n",
    "            self.mf_type = 'Smoothed Triangular'\n",
    "            self.num_antecedents = 3\n",
    "            self.constraints = OrderedConstraint()\n",
    "            self.init_max = 50.0\n",
    "            self.init_min = 0.0\n",
    "        elif mf_type == 'Generalized Bell':\n",
    "            self.mf_type = 'Generalized Bell'\n",
    "\n",
    "            self.num_antecedents = 3\n",
    "            self.constraints = None\n",
    "            self.init_max = 50.0\n",
    "            self.init_min = 1.0\n",
    "\n",
    "        # need to initialize antecedent parameters\n",
    "        self.mf_params = self.add_weight(\n",
    "            shape = (self.num_inputs, self.num_mfs, self.num_antecedents),             \n",
    "            initializer= tf.keras.initializers.RandomUniform(self.init_min, self.init_max),\n",
    "            trainable = True,\n",
    "            name = 'Antecedent_Params',\n",
    "            constraint = self.constraints\n",
    "        )\n",
    "\n",
    "    # custom setting of weights:\n",
    "    def set_weights(self, params):\n",
    "        # this function is used to set weights based on what a user provides\n",
    "        # user must provide weights in the form of a np.array of shape (num_mfs, num_params)\n",
    "\n",
    "        if params.shape != (self.num_inputs, self.num_mfs, self.num_antecedents):\n",
    "            raise ValueError(f'Parameters provided are not of correct shape, expected ({self.num_inputs}, {self.num_mfs}, {self.num_antecedents})')\n",
    "\n",
    "        self.mf_params = params\n",
    "\n",
    "    # function call:\n",
    "    def call(self, inputs):\n",
    "        # need to initialize the membership values:\n",
    "        membership_values = []\n",
    "\n",
    "        # for every input:\n",
    "        for i in range(self.num_inputs):\n",
    "            # get the memberships for that input:\n",
    "            input_mf_params = self.mf_params[i]\n",
    "\n",
    "            # need to now compute the fuzzified value for each membership function:\n",
    "            fuzzified_values = []\n",
    "\n",
    "            # for every membership function:\n",
    "            for j in range(self.num_mfs):\n",
    "\n",
    "                # if gaussian:\n",
    "                if self.mf_type == 'Gaussian':\n",
    "                    # define parameters:\n",
    "                    mean = input_mf_params[j, 0]  # mean of the gaussian\n",
    "                    std = input_mf_params[j, 1]   # standard deviation of the gaussian\n",
    "\n",
    "                    # compute output:\n",
    "                    output = tf.exp(-0.5 * tf.square((inputs[:, i] - mean) / (std + 1e-6)))\n",
    "                    fuzzified_values.append(output)\n",
    "\n",
    "                # if smoothed triangular:\n",
    "                if self.mf_type == 'Smoothed Triangular':\n",
    "                    # define parameters\n",
    "                    a = input_mf_params[j, 0]   # a parameter\n",
    "                    b = input_mf_params[j, 1]   # b parameter\n",
    "                    c = input_mf_params[j, 2]   # c parameter\n",
    "\n",
    "                    # smoothing factor beta:\n",
    "                    beta = 100.0\n",
    "\n",
    "                    # check if we are on the edges:\n",
    "                    is_left_edge = tf.equal(a, b)\n",
    "                    is_right_edge = tf.equal(b, c)\n",
    "\n",
    "                    # compute softplus-based smoothed triangular membership function:\n",
    "                    left = tf.nn.softplus(beta * (inputs[:, i] - a)) / (tf.nn.softplus(beta * (b - a)) + 1e-6)\n",
    "                    right = tf.nn.softplus(beta * (c - inputs[:, i])) / (tf.nn.softplus(beta * (c - b)) + 1e-6)\n",
    "\n",
    "                    # deal with edge case:\n",
    "                    left = tf.where((inputs[:, i] == a) & is_left_edge, 1.0, left)\n",
    "                    right = tf.where((inputs[:, i] == c) & is_right_edge, 1.0, right)\n",
    "\n",
    "                    # compute output:\n",
    "                    output = tf.maximum(0.0, tf.minimum(left, right))\n",
    "                    fuzzified_values.append(output)\n",
    "\n",
    "                # if generalized bell:\n",
    "                if self.mf_type == 'Generalized Bell':\n",
    "                    # define parameters\n",
    "                    a = input_mf_params[j, 0]\n",
    "                    b = input_mf_params[j, 1]\n",
    "                    c = input_mf_params[j, 2]\n",
    "\n",
    "                    # clamp b:\n",
    "                    b = tf.clip_by_value(b, 1e-6, 5.0)\n",
    "\n",
    "                    # compute output:\n",
    "                    output = 1 / (1 + tf.abs((inputs[:, i] - c) / (a + 1e-6)) ** (2 * b))\n",
    "                    fuzzified_values.append(output)\n",
    "            \n",
    "            # need to now stack the mf values for that given input:\n",
    "            membership_values.append(tf.stack(fuzzified_values, axis = -1))\n",
    "\n",
    "        # stack everything and return:\n",
    "        return tf.stack(membership_values, axis = 1)\n",
    "    \n",
    "# second layer -> firing strength layer:\n",
    "class FS_Layer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(FS_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, membership_values):\n",
    "        # this layer accepts the membership values, which have shape (batch_size, num_inputs, num_mfs):\n",
    "        batch_size = tf.shape(membership_values)[0]\n",
    "\n",
    "        # initialize the firing strengths:\n",
    "        firing_strengths = tf.ones((batch_size, self.num_rules), dtype = tf.float32)\n",
    "\n",
    "        # generate all the rule combinations:\n",
    "        rules = list(product(range(self.num_mfs), repeat = self.num_inputs))    # example [(0, 0, 0), (0, 0, 1), ...]\n",
    "\n",
    "        # need to check each input, each mf combination, and multiply their values together:\n",
    "        for rule_index, combination in enumerate(rules):\n",
    "            # print(f'combination: {combination}')\n",
    "            rule_strength = tf.ones((batch_size, ), dtype = tf.float32)\n",
    "\n",
    "            # for every input and membership function:\n",
    "            for input_index, mf_index in enumerate(combination):\n",
    "                # print(f'input: {input_index + 1} | mf: {mf_index + 1}')\n",
    "\n",
    "                # correctly extract the fuzzified values based on the combination index:\n",
    "                rule_strength *= membership_values[:, input_index, mf_index] + 1e-6\n",
    "            \n",
    "            # update the firing strengths:\n",
    "            rule_strength = tf.expand_dims(rule_strength, axis = -1)  # shape: (batch_size, 1)\n",
    "            firing_strengths = tf.concat(\n",
    "                [firing_strengths[:, :rule_index], rule_strength, firing_strengths[:, rule_index + 1:]],\n",
    "                axis = 1,\n",
    "            )\n",
    "            # print(f'firing strength: {firing_strengths}')\n",
    "\n",
    "        return firing_strengths\n",
    "    \n",
    "# third layer -> normalization layer:\n",
    "class NM_Layer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(NM_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, firing_strengths):\n",
    "        # this function accepts inputs of size (batch_size, num_rules).\n",
    "        # need to first get the total firing strength:\n",
    "        total_firing_strength = tf.reduce_sum(firing_strengths, axis = 1, keepdims = True)\n",
    "        \n",
    "        # can now normalize the firing strengths:\n",
    "        normalized_strengths = firing_strengths / (total_firing_strength + 1e-10)   # add a buffer in case the total firing strength is zero\n",
    "\n",
    "        return normalized_strengths\n",
    "    \n",
    "# fourth layer -> consequent layer:\n",
    "class CN_Layer(Layer):\n",
    "    # constructor: \n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(CN_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "        self.num_rules = num_mfs ** num_inputs\n",
    "\n",
    "        # need to initialize the consequent parameters:\n",
    "        self.consequent_params = self.add_weight(\n",
    "            shape = (self.num_rules, self.num_inputs + 1),\n",
    "            initializer = tf.keras.initializers.RandomUniform(-1.0, 1.0, seed = 1234),\n",
    "            trainable = True,\n",
    "            name = 'Consequent_Params'\n",
    "        )\n",
    "\n",
    "    # this function is used for manually setting the consequent parameters:\n",
    "    def set_cons(self, params):\n",
    "        # this function accepts parameters as an array of size (num_rules, num_inputs + 1):\n",
    "        if params.shape != (self.num_rules, self.num_inputs + 1):\n",
    "            raise ValueError(f'Parameters provided are not of correct shape, expected ({self.num_rules}, {self.num_inputs + 1})')\n",
    "        \n",
    "        # assign parameters:\n",
    "        self.consequent_params = params\n",
    "\n",
    "    # call function:\n",
    "    def call(self, input_list):\n",
    "        # unpack inputs from list:\n",
    "        normalized_strengths, inputs = input_list\n",
    "\n",
    "        # get the batch size:\n",
    "        batch_size = tf.shape(normalized_strengths)[0]\n",
    "\n",
    "        # the output is given by the multiplication of the inputs with the consequent weights,\n",
    "        # such as: o_k = w_bar_k * (x_1 * p_k + x_2 * q_k + x_3 * r_k + ... + s_k)\n",
    "        # can therefore extend the inputs to be (batch_size, num_inputs + bias) for ease of multiplication:\n",
    "        inputs_with_bias = tf.concat([inputs, tf.ones((batch_size, 1), dtype = tf.float32)], axis = -1)\n",
    "\n",
    "        # need to now reshape the normalized strengths to be of size (batch_size, num_rules, 1)\n",
    "        # this effectively flips it into a 'column vector' of sorts, where each individual value is now vertically aligned\n",
    "        normalized_strengths = tf.reshape(normalized_strengths, (batch_size, self.num_rules, 1))\n",
    "\n",
    "        # get the consequent parameters, which have shape (num_rules, num_inputs + 1):\n",
    "        consequent_params = self.consequent_params\n",
    "\n",
    "        # expand inputs with bias to match the rule axis: (batch_size, num_rules, num_inputs + 1)\n",
    "        inputs_with_bias_expanded = tf.expand_dims(inputs_with_bias, axis = 1)\n",
    "\n",
    "        # calculate the consequent for each rule\n",
    "        consequents = tf.reduce_sum(normalized_strengths * inputs_with_bias_expanded * consequent_params, axis = 2)\n",
    "\n",
    "        return consequents\n",
    "\n",
    "# fifth layer -> output layer:\n",
    "class O_Layer(Layer):\n",
    "    # constructor:\n",
    "    def __init__(self, num_inputs, num_mfs, **kwargs):\n",
    "        super(O_Layer, self).__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_output = num_mfs\n",
    "\n",
    "    # call function:\n",
    "    def call(self, consequents):\n",
    "        output = tf.reduce_sum(consequents, axis = 1, keepdims = True)\n",
    "        return output\n",
    "    \n",
    "# define a custom function for building models:\n",
    "def BuildAnfis(input_shape, num_inputs, num_mfs, mf_type, rate):\n",
    "    # define the inputs:\n",
    "    inputs = Input(shape = input_shape)\n",
    "\n",
    "    # add the custom layers:\n",
    "    membership_layer = MF_Layer(num_inputs = num_inputs, num_mfs = num_mfs, mf_type = mf_type)(inputs)\n",
    "    firing_layer = FS_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(membership_layer)\n",
    "    normalization_layer = NM_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(firing_layer)\n",
    "    consequent_layer = CN_Layer(num_inputs = num_inputs, num_mfs = num_mfs)([normalization_layer, inputs])\n",
    "    output_layer = O_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(consequent_layer)\n",
    "\n",
    "    # create and compile the model:\n",
    "    model = Model(inputs = inputs, outputs = output_layer)\n",
    "    model.compile(optimizer = Adam(learning_rate = rate, clipvalue = 1.0), \n",
    "                  loss = 'mse', \n",
    "                  metrics = ['mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Importation:**\n",
    "\n",
    "This section imports and processes the data for use in the ANFIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "# import data from csv as pandas dataframe:\n",
    "data = pd.read_csv('V3_Data.csv')\n",
    "print('\\nData loaded sucessfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into X and Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform split:\n",
    "x_data = data.drop(columns = 'Suitability').astype('float32').values\n",
    "y_data = data['Suitability'].astype('float32').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, validation, and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples have shape: (8000, 3)\n",
      "Validation examples have shape: (1000, 3)\n",
      "Testing examples have shape:(1000, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the data using train_test_split:\n",
    "x_train, x_filler, y_train, y_filler = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_filler, y_filler, test_size = 0.5)\n",
    "\n",
    "# get the split results:\n",
    "print(f'Training examples have shape: {x_train.shape}')\n",
    "print(f'Validation examples have shape: {x_val.shape}')\n",
    "print(f'Testing examples have shape:{x_test.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a scaler:\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale each set:\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Exploration:**\n",
    "\n",
    "Within this section a function is defined for instantiating models using the Keras API, for use in performing a hyperparameter search to determine the best combination of hyperparameters. The hyperparametesr that are being considered are:\n",
    "\n",
    "* number of epochs\n",
    "* batch size\n",
    "* learning rate\n",
    "* membership function type\n",
    "* number of membership functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter values to be explored:\n",
    "num_epochs = [50, 100, 250, 500]\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005]\n",
    "membership_functions = ['Smoothed Triangular', 'Gaussian', 'Generalized Bell']\n",
    "num_mfs = [3, 4, 5]\n",
    "\n",
    "combinations = len(num_epochs) * len(batch_sizes) * len(learning_rates) * len(membership_functions) * len(num_mfs)\n",
    "\n",
    "LOSS_FUNCTION = 'mse'\n",
    "METRICS = ['mae', keras.metrics.RootMeanSquaredError(), keras.metrics.R2Score()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model generation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function:\n",
    "def BuildAnfis(input_shape, num_inputs, num_mfs, mf_type, rate):\n",
    "    # define the inputs:\n",
    "    inputs = Input(shape = input_shape)\n",
    "\n",
    "    # add the custom layers:\n",
    "    membership_layer = MF_Layer(num_inputs = num_inputs, num_mfs = num_mfs, mf_type = mf_type)(inputs)\n",
    "    firing_layer = FS_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(membership_layer)\n",
    "    normalization_layer = NM_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(firing_layer)\n",
    "    consequent_layer = CN_Layer(num_inputs = num_inputs, num_mfs = num_mfs)([normalization_layer, inputs])\n",
    "    output_layer = O_Layer(num_inputs = num_inputs, num_mfs = num_mfs)(consequent_layer)\n",
    "\n",
    "    # compile the model:\n",
    "    model = Model(inputs = inputs, outputs = output_layer)\n",
    "    model.compile(optimizer = Adam(learning_rate = rate), \n",
    "                  loss = 'mse', \n",
    "                  metrics = ['mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must perform the grid search. This process entails:\n",
    "\n",
    "* Creating a directory to save the search results in\n",
    "* Creating a model using the aforementioned ***BuildAnfis()*** function\n",
    "* Saving the parameters used in the creation of the model within a dictionary called ***model_params***\n",
    "* Training the model, saving the results into a dictionary called ***training_results***\n",
    "* Combining the training parameters with the training results into a JSON dump\n",
    "\n",
    "While iterating through each combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examining model 720/720\r"
     ]
    }
   ],
   "source": [
    "# define callback: \n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 3, restore_best_weights = True)\n",
    "\n",
    "# begin grid search:\n",
    "j = 1\n",
    "for epochs in num_epochs:\n",
    "    for batch in batch_sizes:\n",
    "        for rate in learning_rates:\n",
    "            for mf in membership_functions:\n",
    "                for num in num_mfs:\n",
    "                    # update user:\n",
    "                    print(f'examining model {j}/{combinations}', end = '\\r')\n",
    "                    j +=1\n",
    "\n",
    "                    # make a directory to save into:\n",
    "                    output_dir = os.path.join(os.getcwd(), f\"anfis_search_results//{str(epochs)}//{str(batch)}//{str(rate)}//{mf}//{str(num)}\")\n",
    "                    os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "                    # build the model:\n",
    "                    tf.keras.backend.clear_session()\n",
    "                    model = BuildAnfis(input_shape = (3, ),\n",
    "                                       num_inputs = 3,\n",
    "                                       num_mfs = num,\n",
    "                                       mf_type = mf,\n",
    "                                       rate = rate)\n",
    "                    \n",
    "                    # save the training parameters into a dictionary:\n",
    "                    training_params = {\n",
    "                        'mf_type' : mf,\n",
    "                        'num_mfs' : num,\n",
    "                        'learning_rate' : rate,\n",
    "                        'batch_size' : batch,\n",
    "                        'num_epochs' : epochs\n",
    "                    }\n",
    "\n",
    "                    # train the model:\n",
    "                    train_start = time.time()\n",
    "                    history = model.fit(x_train, y_train,\n",
    "                                        epochs = epochs,\n",
    "                                        batch_size = batch,\n",
    "                                        validation_data = [x_val, y_val],\n",
    "                                        verbose = 0,\n",
    "                                        callbacks = early_stopping)\n",
    "                    train_time = time.time() - train_start\n",
    "\n",
    "                    # store training results:\n",
    "                    training_results = {}\n",
    "                    for i in history.history.keys():\n",
    "                        training_results[i] = history.history[i][-1]\n",
    "                    training_results['train_time'] = train_time\n",
    "\n",
    "                    # save both results to a directory:\n",
    "                    params_path = os.path.join(output_dir, \"params_results.json\")\n",
    "                    with open(params_path, \"w\") as f:\n",
    "                        json.dump({'parameters': training_params, 'results': training_results}, f, indent = 4)               \n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
