{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "\n",
    "This file serves to design and test a custom implementation of an Adaptive Neuro-Fuzzy Inference System (ANFIS) within Keras. It is hoped that this custom function is able to be used like a standard Keras model, and will be trained and evaluated against the ANN designed previously. \n",
    "\n",
    "This ANFIS takes the load history, the distance to the task, and the total distance travelled thus far and performs inference about the suitability of a given robot for a task at hand. \n",
    "\n",
    "**Date Created:** 13/01/2025\n",
    "\n",
    "**Date Modified:** 14/01/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages:** \n",
    "\n",
    "This section imports all the necessary packages for the ANFIS implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Layer Function & Class Definitions:**\n",
    "\n",
    "Need to define the membership function to be used, as well as the custom layers of an ANFIS for implementation within Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for triangular membership functions:\n",
    "def triangular_mf(x, a, b, c):\n",
    "    return np.maximum(0.0, np.minimum((x - a) / (b - a), (c - x) / (c - b)))\n",
    "\n",
    "# custom fuzzification layer:\n",
    "class FuzzificationLayer:\n",
    "    def __init__(self, num_inputs, num_mfs):\n",
    "        self.num_inputs  = num_inputs\n",
    "        self.num_mfs = num_mfs\n",
    "\n",
    "        # randomly initialize the parameters:\n",
    "        self.params = np.random.uniform(low = -1.0, high = 1.0, size = (num_inputs, num_mfs, 3))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # ensure that inputs are numpy array\n",
    "        inputs = np.array(inputs)\n",
    "\n",
    "        mfs_list = []\n",
    "\n",
    "        for i in range(self.num_inputs):\n",
    "            mfs_for_feature = []\n",
    "            for j in range(self.num_mfs):\n",
    "                # apply the MF to each input feature\n",
    "                mfs_for_feature.append(triangular_mf(inputs[:, i], *self.mf_params[i, j]))\n",
    "            mfs_list.append(np.stack(mfs_for_feature, axis=1))  # stack the MFs for each input\n",
    "        \n",
    "        # concatenate all the MFs to form the output\n",
    "        return np.concatenate(mfs_list, axis=1)\n",
    "    \n",
    "# custom firing strength layer:\n",
    "class FiringStrengthLayer:\n",
    "    def __init__(self, num_rules):\n",
    "        self.num_rules = num_rules\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return np.prod(inputs, axis = 1)\n",
    "    \n",
    "# custom normalization layer:\n",
    "class NormalizationLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def call(self, firing_strengths):\n",
    "        total_strength = np.sum(firing_strengths, axis = 1, keepdims = True)\n",
    "        return firing_strengths / total_strength\n",
    "    \n",
    "# custom rule consequent layer:\n",
    "class ConsequentLayer:\n",
    "    def __init__(self, num_rules, num_inputs):\n",
    "            self.num_rules = num_rules\n",
    "            self.num_inputs = num_inputs\n",
    "\n",
    "            # initialize parameters for the consequents (weights and bias for each rule)\n",
    "            self.params = np.random.uniform(low=-1.0, high=1.0, size=(num_rules, num_inputs + 1))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch_size, num_rules), where each column is the normalized firing strength for a rule\n",
    "        return np.dot(inputs, self.params.T)  # shape: (batch_size, num_rules)\n",
    "    \n",
    "# custom output layer:\n",
    "class OutputLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def call(self, rule_consequents, firing_strengths):\n",
    "        # weighted sum of the rule consequents\n",
    "        return np.sum(rule_consequents * firing_strengths[:, None], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer made\n"
     ]
    }
   ],
   "source": [
    "fl = FuzzificationLayer(num_inputs = 3, num_mfs = 3)\n",
    "print('layer made')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define Testing Parameters:**\n",
    "\n",
    "This section defines the testing parameters, such as the number of inputs, the number of membership functions, and the expected number of rules. This section also defines a function for creating and compiling Keras models using the custom layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters:\n",
    "num_inputs = 3\n",
    "num_mfs = 3\n",
    "num_rules = num_mfs ** num_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Generation Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make models:\n",
    "def make_model(num_inputs, num_mfs, num_rules, rate):\n",
    "\n",
    "    # instantiate model:\n",
    "    model = Sequential()\n",
    "\n",
    "    # add fuzzification layer:\n",
    "    model.add(FuzzificationLayer(num_inputs = num_inputs, num_mfs = num_mfs))\n",
    "\n",
    "    # add firing strength layer:\n",
    "    model.add(FiringStrengthLayer())\n",
    "\n",
    "    # add normalization layer:\n",
    "    model.add(NormalizationLayer())\n",
    "\n",
    "    # add rule consequent layer:\n",
    "    model.add(ConsequentLayer(num_rules = num_rules, num_inputs = num_inputs))\n",
    "\n",
    "    # add output layer:\n",
    "    model.add(OutputLayer())\n",
    "    \n",
    "    # return model:\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define Hybrid Training Function:**\n",
    "\n",
    "It is important to note that training an ANFIS involves the use of a hybrid training algorithm usually, where the antecedent parameters (membership function parameters) are updated using back propagation through gradient descent, whereas the consequent parameters (rule consequent parameters) are updated using least-square optimization. This isn't natively supported within Keras, so it must be defined as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that are used in the hybrid training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to compute the forward pass loss (mse):\n",
    "def compute_loss(model, x_batch, y_batch):\n",
    "    y_pred = model(x_batch)\n",
    "    loss = tf.reduce_mean(tf.square(y_pred - y_batch))\n",
    "    return loss\n",
    "\n",
    "# define function to compute validation loss:\n",
    "def compute_val_loss(model, val_dataset):\n",
    "    val_loss = 0\n",
    "    num_batches = 0\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        loss = compute_loss(model, x_batch_val, y_batch_val)\n",
    "        val_loss += loss.numpy()\n",
    "        num_batches += 1\n",
    "    return val_loss / num_batches\n",
    "\n",
    "# define function to update the consequent parameters:\n",
    "def update_consequents(firing_strengths, targets):\n",
    "    x = firing_strengths\n",
    "    y = targets\n",
    "\n",
    "    # least squares solution:\n",
    "    x_t = tf.transpose(x)\n",
    "    inverse_term = tf.linalg.inv(tf.matmul(x_t, x))\n",
    "    params = tf.matmul(inverse_term, tf.matmul(x_t, y))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the function:\n",
    "# def hybrid_train(model, train_dataset, val_dataset, epochs, rate, patience = 5):\n",
    "#     optimizer = Adam(learning_rate = rate)  # define optimizer \n",
    "#     best_val_loss = np.inf                  # initialize best validation loss\n",
    "#     no_improvement_count = 0\n",
    "\n",
    "#     # for every epoch:\n",
    "#     for epoch in range(epochs):\n",
    "#         print(f'epoch {epoch + 1}/{epoch}') # update user\n",
    "\n",
    "#         # training phase:\n",
    "#         for x_batch_train, y_batch_train in train_dataset:\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 # forward pass:\n",
    "#                 fuzzified_inputs = model.layers[0](x_batch_train)\n",
    "#                 firing_strengths = model.layers[1](fuzzified_inputs)\n",
    "#                 normalized_strengths = model.layers[2](firing_strengths)\n",
    "#                 # consequent_outputs = model.layers[3](normalized_strengths)\n",
    "\n",
    "#                 loss = compute_loss(model, x_batch_train, y_batch_train)\n",
    "            \n",
    "#         # backpropagation - need to compute the gradients of the antecedent parameters:\n",
    "#         grads = tape.gradient(loss, model.layers[0].trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(grads, model.layers[0].trainable_variables))\n",
    "\n",
    "#         # now do least squares estimation:\n",
    "#         firing_strengths_reshaped = tf.reshape(firing_strengths, (-1, firing_strengths.shape[-1]))\n",
    "#         consequent_params = update_consequents(firing_strengths_reshaped, y_batch_train)\n",
    "#         model.layers[3].params_assign(consequent_params)\n",
    "\n",
    "#         # validation phase\n",
    "#         val_loss = compute_val_loss(model, val_dataset)\n",
    "#         print(f\"validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "#         # early stopping\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             no_improvement_count = 0  # reset counter if there's improvement\n",
    "#         else:\n",
    "#             no_improvement_count += 1\n",
    "#             if no_improvement_count >= patience:\n",
    "#                 print(\"early stopping triggered due to no improvement in validation loss.\")\n",
    "#                 break\n",
    "\n",
    "#     print(\"training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading of Training Data:**\n",
    "\n",
    "Need to load the data that had been generated using the FIS, such that it may be used to train the ANFIS model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 8000 training examples\n",
      "there are 1000 validation examples\n",
      "there are 1000 testing examples\n"
     ]
    }
   ],
   "source": [
    "# load data from the CSV:\n",
    "df = pd.read_csv('V3_Data.csv')\n",
    "\n",
    "# extract the X and Y components of the dataframe:\n",
    "x_data = df.drop(['Suitability'], axis = 1)\n",
    "y_data = df['Suitability']\n",
    "\n",
    "# need to split the data into training, validation, and testing:\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size = 0.5)\n",
    "\n",
    "# get split results:\n",
    "print(f\"there are {x_train.shape[0]} training examples\")\n",
    "print(f\"there are {x_val.shape[0]} validation examples\")\n",
    "print(f\"there are {x_test.shape[0]} testing examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to perform normalization to improve model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the training data:\n",
    "x_min = x_train.min(axis = 0)\n",
    "x_max = x_train.max(axis = 0)\n",
    "x_train = (x_train - x_min) / (x_max - x_min)\n",
    "\n",
    "# apply the normalization to the validation and testing data:\n",
    "x_val = (x_val - x_min) / (x_max - x_min)\n",
    "x_test = (x_test - x_min) / (x_max - x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to TensorFlow tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor conversion:\n",
    "x_train_tensor = tf.convert_to_tensor(x_train, dtype = tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train, dtype = tf.float32)\n",
    "x_val_tensor = tf.convert_to_tensor(x_val, dtype = tf.float32)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val, dtype = tf.float32)\n",
    "x_test_tensor = tf.convert_to_tensor(x_test, dtype = tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test, dtype = tf.float32)\n",
    "\n",
    "# batch the data:\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_tensor, y_train_tensor)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val_tensor, y_val_tensor)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_tensor, y_test_tensor)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train the Model:**\n",
    "\n",
    "Use the hybrid training model to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/0\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling FuzzificationLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'fuzzification_layer_17' (of type FuzzificationLayer). Either the `FuzzificationLayer.call()` method is incorrect, or you need to implement the `FuzzificationLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by FuzzificationLayer.call():\n  • args=('<KerasTensor shape=(32, 3), dtype=float32, sparse=False, name=keras_tensor_3>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m make_model(num_inputs \u001b[38;5;241m=\u001b[39m num_inputs, num_mfs \u001b[38;5;241m=\u001b[39m num_mfs, num_rules \u001b[38;5;241m=\u001b[39m num_rules, rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# train the model using the hybrid algorithm:\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mhybrid_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[145], line 20\u001b[0m, in \u001b[0;36mhybrid_train\u001b[1;34m(model, train_dataset, val_dataset, epochs, rate, patience)\u001b[0m\n\u001b[0;32m     17\u001b[0m         normalized_strengths \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m2\u001b[39m](firing_strengths)\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# consequent_outputs = model.layers[3](normalized_strengths)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# backpropagation - need to compute the gradients of the antecedent parameters:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "Cell \u001b[1;32mIn[144], line 3\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(model, x_batch, y_batch)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(model, x_batch, y_batch):\n\u001b[1;32m----> 3\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(y_pred \u001b[38;5;241m-\u001b[39m y_batch))\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\mtidd2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[141], line 34\u001b[0m, in \u001b[0;36mFuzzificationLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     32\u001b[0m     mf_for_feature \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_mfs):\n\u001b[1;32m---> 34\u001b[0m         mf_for_feature\u001b[38;5;241m.\u001b[39mappend(triangular_mf(input_feature, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmf_params[i, j]))  \u001b[38;5;66;03m# Apply MF for feature 'i' and MF 'j'\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     mfs_list\u001b[38;5;241m.\u001b[39mappend(tf\u001b[38;5;241m.\u001b[39mstack(mf_for_feature, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Stack results for each MF (shape: (batch_size, num_mfs))\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Concatenate the results for all features along the feature axis\u001b[39;00m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling FuzzificationLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'fuzzification_layer_17' (of type FuzzificationLayer). Either the `FuzzificationLayer.call()` method is incorrect, or you need to implement the `FuzzificationLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by FuzzificationLayer.call():\n  • args=('<KerasTensor shape=(32, 3), dtype=float32, sparse=False, name=keras_tensor_3>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "# first need to instantiate a model:\n",
    "model = make_model(num_inputs = num_inputs, num_mfs = num_mfs, num_rules = num_rules, rate = 0.001)\n",
    "\n",
    "# train the model using the hybrid algorithm:\n",
    "hybrid_train(model, train_dataset, val_dataset, 50, 0.001, patience = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
