{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "\n",
    "This file serves to design and test a custom implementation of an Adaptive Neuro-Fuzzy Inference System (ANFIS) within Keras. It is hoped that this custom function is able to be used like a standard Keras model, and will be trained and evaluated against the ANN designed previously. \n",
    "\n",
    "This ANFIS takes the load history, the distance to the task, and the total distance travelled thus far and performs inference about the suitability of a given robot for a task at hand. \n",
    "\n",
    "**Date Created:** 13/01/2025\n",
    "\n",
    "**Date Modified:** 14/01/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Packages:** \n",
    "\n",
    "This section imports all the necessary packages for the ANFIS implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Layer Function & Class Definitions:**\n",
    "\n",
    "Need to define the membership function to be used, as well as the custom layers of an ANFIS for implementation within Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for triangular membership functions:\n",
    "def triangular_mf(x, a, b, c):\n",
    "    return tf.maximum(0.0, tf.minimum((x - a) / (b - a), (c - x) / (c - b)))\n",
    "\n",
    "# custom fuzzification layer:\n",
    "class FuzzificationLayer(layers.Layer):\n",
    "\n",
    "    # constructor to initialize objects:\n",
    "    def __init__(self, num_inputs, num_mfs):\n",
    "        super(FuzzificationLayer, self).__init__()  # initialize in the same manner as the parent\n",
    "        self.num_inputs = num_inputs                # assign the number of inputs\n",
    "        self.num_mfs = num_mfs                      # assign the number of membership functions\n",
    "\n",
    "        # create learnable mf parameters\n",
    "        self.mf_params = self.add_weight(           \n",
    "            shape = (num_inputs, num_mfs, 3),       # shape being num_inputs rows, num_mfs columns, and a depth of 3 for the 3 parameters in a triangular membership function\n",
    "            initializer = 'random_uniform',         # randomly, uniformly initialize the weights\n",
    "            trainable = True,                       # set to be trainable\n",
    "        )\n",
    "\n",
    "    # forward pass through layer given by:\n",
    "    def call(self, inputs):\n",
    "        mfs = []            # initialize empty list\n",
    "        for i in range(self.num_inputs):        # for each input x_i\n",
    "            mfs.append(                         # add the output to the mf list\n",
    "                tf.stack(\n",
    "                    # each input i across all instances is inputted, along with the parameters for the j-th membership function for the i-th input\n",
    "                    [triangular_mf(inputs[:,i], *self.mf_params[i, j]) for j in range(self.num_mfs)], axis = 1\n",
    "                )\n",
    "            )\n",
    "        return tf.concat(mfs, axis = 1) # return to user\n",
    "    \n",
    "# custom firing strength layer:\n",
    "class FiringStrengthLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        rules = tf.reduce_prod(inputs, axis = 1, keepdims = True)\n",
    "        return rules\n",
    "    \n",
    "# custom normalization layer:\n",
    "class NormalizationLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return inputs / tf.reduce_sum(inputs, axis = 1, keepdims = True)\n",
    "    \n",
    "# custom consequent layer:\n",
    "class ConsequentLayer(layers.Layer):\n",
    "\n",
    "    # constructor to initialize objects:\n",
    "    def __init__(self, num_rules, num_inputs):\n",
    "        super(ConsequentLayer, self).__init__()     # initialize in the same manner as the parent\n",
    "        self.params = self.add_weight(              # takagi-sugeno consequent parameters\n",
    "            shape = (num_rules, num_inputs + 1),\n",
    "            initializer = 'random_uniform',\n",
    "            trainable = True\n",
    "        )\n",
    "\n",
    "    # forward pass through layer given by:\n",
    "    def call(self, inputs):\n",
    "        # add a bias term to the inputs:\n",
    "        bias = tf.ones((inputs.shape[0], 1))\n",
    "        extended_inputs = tf.concat([inputs, bias], axis = 1)\n",
    "        return tf.matmul(extended_inputs, self.params, transpose_b=True)\n",
    "    \n",
    "# custom output layer:\n",
    "class OutputLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_sum(inputs, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define Testing Parameters:**\n",
    "\n",
    "This section defines the testing parameters, such as the number of inputs, the number of membership functions, and the expected number of rules. This section also defines a function for creating and compiling Keras models using the custom layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters:\n",
    "num_inputs = 3\n",
    "num_mfs = 3\n",
    "num_rules = num_mfs ** num_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Generation Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make models:\n",
    "def make_model(num_inputs, num_mfs, num_rules, rate):\n",
    "\n",
    "    # instantiate model:\n",
    "    model = Sequential()\n",
    "\n",
    "    # add fuzzification layer:\n",
    "    model.add(FuzzificationLayer(num_inputs = num_inputs, num_mfs = num_mfs))\n",
    "\n",
    "    # add firing strength layer:\n",
    "    model.add(FiringStrengthLayer())\n",
    "\n",
    "    # add normalization layer:\n",
    "    model.add(NormalizationLayer())\n",
    "\n",
    "    # add rule consequent layer:\n",
    "    model.add(ConsequentLayer(num_rules = num_rules, num_inputs = num_inputs))\n",
    "\n",
    "    # add output layer:\n",
    "    model.add(OutputLayer())\n",
    "    \n",
    "    # return model:\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define Hybrid Training Function:**\n",
    "\n",
    "It is important to note that training an ANFIS involves the use of a hybrid training algorithm usually, where the antecedent parameters (membership function parameters) are updated using back propagation through gradient descent, whereas the consequent parameters (rule consequent parameters) are updated using least-square optimization. This isn't natively supported within Keras, so it must be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hybrid training function:\n",
    "\n",
    "def hybrid_train(model, train_dataset, num_epochs, rate):\n",
    "    # define an optimizer:\n",
    "    optimizer = Adam(learning_rate = rate)\n",
    "\n",
    "    # for every epoch:\n",
    "    for epoch in range(num_epochs):\n",
    "        # for every x,y batch in the dataset:\n",
    "        for x_batch, y_batch in train_dataset:\n",
    "            # first must use backpropagation for antecedents:\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(x_batch, training = True)                   # get forward pass output\n",
    "                loss = tf.reduce_mean(tf.square(y_batch - outputs))         # calculate MSE \n",
    "\n",
    "            # gradient descent:\n",
    "            antecedent_variables = model.layers[0].trainable_variables      # access trainable variables of fuzzification layer\n",
    "            grads = tape.gradient(loss, antecedent_variables)               # get the gradient\n",
    "            optimizer.apply_gradients(zip(grads, antecedent_variables))     # apply the gradient to the variables\n",
    "\n",
    "            # now we can do least squares optimization for consequents:\n",
    "            firing_strengths = model.layers[1](model.layers[0](x_batch))    # firing strength layer output\n",
    "            normalized_strengths = model.layers[2](firing_strengths)        # normalized firing strength layer output\n",
    "            extended_inputs = tf.concat([normalized_strengths, tf.ones_like(normalized_strengths)], axis = 1)\n",
    "\n",
    "            # solve for consequents using least-squares:\n",
    "            consequent_weights = tf.linalg.lstsq(extended_inputs, y_batch, l2_regularizer=1e-3)\n",
    "            model.layers[3].params.assign(tf.transpose(consequent_weights)) # update consequent parameters\n",
    "        \n",
    "        # logging the loss for monitoring\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.numpy():.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading of Training Data:**\n",
    "\n",
    "Need to load the data that had been generated using the FIS, such that it may be used to train the ANFIS model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 8000 training examples\n",
      "there are 1000 validation examples\n",
      "there are 1000 testing examples\n"
     ]
    }
   ],
   "source": [
    "# load data from the CSV:\n",
    "df = pd.read_csv('V3_Data.csv')\n",
    "\n",
    "# extract the X and Y components of the dataframe:\n",
    "x_data = df.drop(['Suitability'], axis = 1)\n",
    "y_data = df['Suitability']\n",
    "\n",
    "# need to split the data into training, validation, and testing:\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size = 0.5)\n",
    "\n",
    "# get split results:\n",
    "print(f\"there are {x_train.shape[0]} training examples\")\n",
    "print(f\"there are {x_val.shape[0]} validation examples\")\n",
    "print(f\"there are {x_test.shape[0]} testing examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to perform normalization to improve model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the training data:\n",
    "x_min = x_train.min(axis = 0)\n",
    "x_max = x_train.max(axis = 0)\n",
    "x_train = (x_train - x_min) / (x_max - x_min)\n",
    "\n",
    "# apply the normalization to the validation and testing data:\n",
    "x_val = (x_val - x_min) / (x_max - x_min)\n",
    "x_test = (x_test - x_min) / (x_max - x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to TensorFlow tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor conversion:\n",
    "x_train_tensor = tf.convert_to_tensor(x_train, dtype = tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train, dtype = tf.float32)\n",
    "x_val_tensor = tf.convert_to_tensor(x_val, dtype = tf.float32)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val, dtype = tf.float32)\n",
    "x_test_tensor = tf.convert_to_tensor(x_test, dtype = tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test, dtype = tf.float32)\n",
    "\n",
    "# batch the data:\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_tensor, y_train_tensor)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val_tensor, y_val_tensor)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_tensor, y_test_tensor)).batch(batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
